{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing modules and data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:00:34.692420Z",
     "start_time": "2019-03-17T15:00:31.676147Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:00:47.483479Z",
     "start_time": "2019-03-17T15:00:47.473273Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = str.maketrans({key: None for key in string.punctuation})\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:00:52.825616Z",
     "start_time": "2019-03-17T15:00:52.822555Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:10:26.726187Z",
     "start_time": "2019-03-17T15:10:26.661056Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection.csv\",delimiter=\"\\t\") #Reading data\n",
    "data.columns = ['Type','Text'] #Changing Column names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:10:27.119844Z",
     "start_time": "2019-03-17T15:10:27.107980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type                                               Text\n",
       "0   ham                      Ok lar... Joking wif u oni...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  U dun say so early hor... U c already then say...\n",
       "3   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "4  spam  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5) #Displays first five records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:15:33.793754Z",
     "start_time": "2019-03-17T15:15:33.785181Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = re.sub('[\\d]','',text)\n",
    "    text = text.translate(table)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:15:34.499983Z",
     "start_time": "2019-03-17T15:15:34.303635Z"
    }
   },
   "outputs": [],
   "source": [
    "data['Text'] = data['Text'].apply(lambda x : preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:15:34.941526Z",
     "start_time": "2019-03-17T15:15:34.925818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in  a wkly comp to win FA Cup final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling its been  weeks now ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type                                               Text\n",
       "0   ham                            Ok lar Joking wif u oni\n",
       "1  spam  Free entry in  a wkly comp to win FA Cup final...\n",
       "2   ham        U dun say so early hor U c already then say\n",
       "3   ham  Nah I dont think he goes to usf he lives aroun...\n",
       "4  spam  FreeMsg Hey there darling its been  weeks now ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constructing tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:15:38.094547Z",
     "start_time": "2019-03-17T15:15:38.082992Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:15:54.743270Z",
     "start_time": "2019-03-17T15:15:54.350065Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:19:36.877270Z",
     "start_time": "2019-03-17T15:19:36.858449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:19:38.202199Z",
     "start_time": "2019-03-17T15:19:38.186773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:19:38.712106Z",
     "start_time": "2019-03-17T15:19:38.698355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5571, 8339)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:24:33.357576Z",
     "start_time": "2019-03-17T15:24:33.342296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5571x8339 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 41245 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:22:01.336435Z",
     "start_time": "2019-03-17T15:22:00.714484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrixlib.defmatrix.matrix"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense_mat = X_train_tfidf.todense()\n",
    "\n",
    "type(Dense_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:24:44.540801Z",
     "start_time": "2019-03-17T15:24:44.529667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:22:01.589937Z",
     "start_time": "2019-03-17T15:22:01.524760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8339"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_transformer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:23:58.540106Z",
     "start_time": "2019-03-17T15:23:58.529267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8339"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:22:02.590852Z",
     "start_time": "2019-03-17T15:22:02.484178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aah',\n",
       " 'aaniye',\n",
       " 'aaooooright',\n",
       " 'aathilove',\n",
       " 'aathiwhere',\n",
       " 'ab',\n",
       " 'abbey',\n",
       " 'abdomen',\n",
       " 'abeg',\n",
       " 'abelu',\n",
       " 'aberdeen',\n",
       " 'abi',\n",
       " 'ability',\n",
       " 'abiola',\n",
       " 'abj',\n",
       " 'able',\n",
       " 'abnormally',\n",
       " 'aboutas',\n",
       " 'abroad',\n",
       " 'absence',\n",
       " 'absolutely',\n",
       " 'abstract',\n",
       " 'abt',\n",
       " 'abta',\n",
       " 'aburo',\n",
       " 'abuse',\n",
       " 'abusers',\n",
       " 'ac',\n",
       " 'academic',\n",
       " 'acc',\n",
       " 'accent',\n",
       " 'accenture',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accidant',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accommodation',\n",
       " 'accommodationvouchers',\n",
       " 'accomodate',\n",
       " 'accomodations',\n",
       " 'accordin',\n",
       " 'accordingly',\n",
       " 'accordinglyor',\n",
       " 'account',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accumulation',\n",
       " 'achanammarakheshqatar',\n",
       " 'ache',\n",
       " 'achieve',\n",
       " 'acid',\n",
       " 'acknowledgement',\n",
       " 'aclpm',\n",
       " 'acnt',\n",
       " 'acoentry',\n",
       " 'acsmsrewards',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'activities',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acwicmbcktzr',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'addamsfa',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addie',\n",
       " 'adding',\n",
       " 'address',\n",
       " 'addressull',\n",
       " 'adds',\n",
       " 'adewale',\n",
       " 'adi',\n",
       " 'adjustable',\n",
       " 'admin',\n",
       " 'administrator',\n",
       " 'admirer',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admiti',\n",
       " 'adore',\n",
       " 'adoring',\n",
       " 'adp',\n",
       " 'adress',\n",
       " 'adrian',\n",
       " 'adrink',\n",
       " 'ads',\n",
       " 'adsense',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'adventure',\n",
       " 'adventuring',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advising',\n",
       " 'advisors',\n",
       " 'ae',\n",
       " 'aeronautics',\n",
       " 'aeroplane',\n",
       " 'afew',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affectionsamp',\n",
       " 'affidavit',\n",
       " 'afford',\n",
       " 'afghanistan',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'aft',\n",
       " 'afternon',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'aftr',\n",
       " 'ag',\n",
       " 'againcall',\n",
       " 'againloving',\n",
       " 'agalla',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ageppermesssubscription',\n",
       " 'ages',\n",
       " 'agesring',\n",
       " 'agidhane',\n",
       " 'aging',\n",
       " 'ago',\n",
       " 'agocusoon',\n",
       " 'agree',\n",
       " 'agreen',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahgee',\n",
       " 'ahhh',\n",
       " 'ahhhhjust',\n",
       " 'ahmad',\n",
       " 'ahnow',\n",
       " 'ahold',\n",
       " 'ahsen',\n",
       " 'ahthe',\n",
       " 'ahwhat',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'aig',\n",
       " 'aight',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'airtel',\n",
       " 'aiya',\n",
       " 'aiyah',\n",
       " 'aiyar',\n",
       " 'aiyo',\n",
       " 'aj',\n",
       " 'ajith',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akonlonely',\n",
       " 'al',\n",
       " 'alaikkumpride',\n",
       " 'alaipayuthe',\n",
       " 'albi',\n",
       " 'album',\n",
       " 'albumquite',\n",
       " 'alcohol',\n",
       " 'aldrine',\n",
       " 'alert',\n",
       " 'alertfrom',\n",
       " 'alerts',\n",
       " 'aletter',\n",
       " 'alex',\n",
       " 'alexs',\n",
       " 'alfie',\n",
       " 'algarve',\n",
       " 'algebra',\n",
       " 'algorithms',\n",
       " 'ali',\n",
       " 'alian',\n",
       " 'alibi',\n",
       " 'alive',\n",
       " 'alivebetter',\n",
       " 'allah',\n",
       " 'allahmeet',\n",
       " 'allahrakhesh',\n",
       " 'allalo',\n",
       " 'allday',\n",
       " 'alle',\n",
       " 'allo',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'alls',\n",
       " 'alot',\n",
       " 'alreadysabarish',\n",
       " 'alright',\n",
       " 'alrightokay',\n",
       " 'alrite',\n",
       " 'alritehave',\n",
       " 'alsoor',\n",
       " 'alter',\n",
       " 'alternativehope',\n",
       " 'alwa',\n",
       " 'alwys',\n",
       " 'amanda',\n",
       " 'amazing',\n",
       " 'ambitious',\n",
       " 'ambrithmaduraimet',\n",
       " 'american',\n",
       " 'ami',\n",
       " 'amigos',\n",
       " 'amk',\n",
       " 'ammaelife',\n",
       " 'ammo',\n",
       " 'amnow',\n",
       " 'amp',\n",
       " 'amplikater',\n",
       " 'ampm',\n",
       " 'amrca',\n",
       " 'amrita',\n",
       " 'ams',\n",
       " 'amt',\n",
       " 'amused',\n",
       " 'amx',\n",
       " 'amy',\n",
       " 'ana',\n",
       " 'anal',\n",
       " 'analysis',\n",
       " 'anand',\n",
       " 'anderson',\n",
       " 'andor',\n",
       " 'andre',\n",
       " 'andres',\n",
       " 'andrewsboy',\n",
       " 'andros',\n",
       " 'anetworks',\n",
       " 'angels',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'animation',\n",
       " 'anjie',\n",
       " 'anjolas',\n",
       " 'anna',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'annoncement',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'annoyin',\n",
       " 'annoying',\n",
       " 'anonymous',\n",
       " 'anot',\n",
       " 'ans',\n",
       " 'ansr',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answerin',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'answr',\n",
       " 'antelope',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'antibiotic',\n",
       " 'anybody',\n",
       " 'anybodys',\n",
       " 'anymore',\n",
       " 'anyones',\n",
       " 'anyplaces',\n",
       " 'anythiing',\n",
       " 'anythin',\n",
       " 'anythings',\n",
       " 'anythingtomorrow',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'aom',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apes',\n",
       " 'apeshit',\n",
       " 'aphex',\n",
       " 'apnt',\n",
       " 'apo',\n",
       " 'apologetic',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appendix',\n",
       " 'applebees',\n",
       " 'appledayno',\n",
       " 'applespairsall',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'applyed',\n",
       " 'applying',\n",
       " 'appointment',\n",
       " 'appointments',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'apps',\n",
       " 'appt',\n",
       " 'appy',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'aproach',\n",
       " 'apt',\n",
       " 'aptitude',\n",
       " 'aquarius',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabian',\n",
       " 'arcade',\n",
       " 'archive',\n",
       " 'ard',\n",
       " 'ardé',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'arestaurant',\n",
       " 'aretaking',\n",
       " 'argentina',\n",
       " 'argh',\n",
       " 'argue',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'aries',\n",
       " 'arise',\n",
       " 'arises',\n",
       " 'arithmetic',\n",
       " 'arm',\n",
       " 'armand',\n",
       " 'armands',\n",
       " 'armenia',\n",
       " 'arms',\n",
       " 'arng',\n",
       " 'arngd',\n",
       " 'arnt',\n",
       " 'aroundn',\n",
       " 'arpraveesh',\n",
       " 'arr',\n",
       " 'arrange',\n",
       " 'arranging',\n",
       " 'arrested',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrow',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'artists',\n",
       " 'arts',\n",
       " 'arty',\n",
       " 'arul',\n",
       " 'arun',\n",
       " 'asa',\n",
       " 'asap',\n",
       " 'asapok',\n",
       " 'asda',\n",
       " 'ashes',\n",
       " 'ashley',\n",
       " 'ashleys',\n",
       " 'ashwini',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'askd',\n",
       " 'asked',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'aslamalaikkuminsha',\n",
       " 'asleep',\n",
       " 'aspects',\n",
       " 'ass',\n",
       " 'assessment',\n",
       " 'asshole',\n",
       " 'assistance',\n",
       " 'associate',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'asthere',\n",
       " 'asthma',\n",
       " 'astne',\n",
       " 'astoundingly',\n",
       " 'astrology',\n",
       " 'astronomer',\n",
       " 'asus',\n",
       " 'asusual',\n",
       " 'ate',\n",
       " 'athletic',\n",
       " 'athome',\n",
       " 'atlanta',\n",
       " 'atlast',\n",
       " 'atleast',\n",
       " 'atm',\n",
       " 'atrocious',\n",
       " 'attach',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'atten',\n",
       " 'attend',\n",
       " 'attended',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attractioni',\n",
       " 'attractive',\n",
       " 'attracts',\n",
       " 'attributed',\n",
       " 'atyour',\n",
       " 'auction',\n",
       " 'auctionpunj',\n",
       " 'audiitions',\n",
       " 'audition',\n",
       " 'audrey',\n",
       " 'audreys',\n",
       " 'audrie',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'auntie',\n",
       " 'aunties',\n",
       " 'aunts',\n",
       " 'aunty',\n",
       " 'auntys',\n",
       " 'aust',\n",
       " 'australia',\n",
       " 'authorise',\n",
       " 'auto',\n",
       " 'autocorrect',\n",
       " 'av',\n",
       " 'ava',\n",
       " 'availa',\n",
       " 'available',\n",
       " 'availablei',\n",
       " 'availablethey',\n",
       " 'avalarr',\n",
       " 'avatar',\n",
       " 'avble',\n",
       " 'ave',\n",
       " 'avenge',\n",
       " 'avent',\n",
       " 'avenue',\n",
       " 'avin',\n",
       " 'avo',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'avoids',\n",
       " 'await',\n",
       " 'awaiting',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awkward',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'ax',\n",
       " 'axis',\n",
       " 'ay',\n",
       " 'ayn',\n",
       " 'ayo',\n",
       " 'ba',\n",
       " 'baaaaaaaabe',\n",
       " 'baaaaabe',\n",
       " 'babe',\n",
       " 'babeprobpop',\n",
       " 'babes',\n",
       " 'babesozi',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babygoodbye',\n",
       " 'babyhope',\n",
       " 'babyjontet',\n",
       " 'babysit',\n",
       " 'babysitting',\n",
       " 'bac',\n",
       " 'backa',\n",
       " 'backdoor',\n",
       " 'backwards',\n",
       " 'bad',\n",
       " 'badass',\n",
       " 'badly',\n",
       " 'badrith',\n",
       " 'bag',\n",
       " 'bagi',\n",
       " 'bags',\n",
       " 'bahamas',\n",
       " 'baig',\n",
       " 'bailiff',\n",
       " 'bajarangabali',\n",
       " 'bak',\n",
       " 'bakra',\n",
       " 'bakrid',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'baller',\n",
       " 'balloon',\n",
       " 'balls',\n",
       " 'bam',\n",
       " 'bambling',\n",
       " 'band',\n",
       " 'bandages',\n",
       " 'bang',\n",
       " 'bangb',\n",
       " 'bangbabes',\n",
       " 'bani',\n",
       " 'bank',\n",
       " 'banks',\n",
       " 'banned',\n",
       " 'banneduk',\n",
       " 'bannfwflyppm',\n",
       " 'banter',\n",
       " 'bao',\n",
       " 'bar',\n",
       " 'barbie',\n",
       " 'barcelona',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bari',\n",
       " 'barkleys',\n",
       " 'barmed',\n",
       " 'barolla',\n",
       " 'barred',\n",
       " 'barrel',\n",
       " 'barring',\n",
       " 'barry',\n",
       " 'bars',\n",
       " 'base',\n",
       " 'bash',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basketball',\n",
       " 'baskets',\n",
       " 'basqihave',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'batchlor',\n",
       " 'bath',\n",
       " 'bathe',\n",
       " 'bathing',\n",
       " 'bathroom',\n",
       " 'batsman',\n",
       " 'batt',\n",
       " 'battery',\n",
       " 'battle',\n",
       " 'bawling',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bbc',\n",
       " 'bbdeluxe',\n",
       " 'bbdpooja',\n",
       " 'bbdthts',\n",
       " 'bblue',\n",
       " 'bbq',\n",
       " 'bbs',\n",
       " 'bc',\n",
       " 'bcaz',\n",
       " 'bck',\n",
       " 'bcm',\n",
       " 'bcmsfwcnxx',\n",
       " 'bcmwcnxx',\n",
       " 'bcoz',\n",
       " 'bcozi',\n",
       " 'bcum',\n",
       " 'bcums',\n",
       " 'bcz',\n",
       " 'bday',\n",
       " 'beach',\n",
       " 'beads',\n",
       " 'bear',\n",
       " 'bears',\n",
       " 'beatings',\n",
       " 'beauties',\n",
       " 'beautiful',\n",
       " 'beautifulmay',\n",
       " 'beauty',\n",
       " 'bec',\n",
       " 'becaus',\n",
       " 'becausethey',\n",
       " 'becoz',\n",
       " 'becz',\n",
       " 'bed',\n",
       " 'bedbut',\n",
       " 'bedreal',\n",
       " 'bedrm',\n",
       " 'bedroom',\n",
       " 'bedroomlove',\n",
       " 'beeen',\n",
       " 'beehoon',\n",
       " 'beendropping',\n",
       " 'beer',\n",
       " 'beerage',\n",
       " 'beerrs',\n",
       " 'beers',\n",
       " 'befor',\n",
       " 'beforewent',\n",
       " 'beg',\n",
       " 'beggar',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'begins',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behave',\n",
       " 'bein',\n",
       " 'believe',\n",
       " 'belive',\n",
       " 'bell',\n",
       " 'bellearlier',\n",
       " 'belligerent',\n",
       " 'belly',\n",
       " 'belong',\n",
       " 'belongs',\n",
       " 'belovd',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'bend',\n",
       " 'beneath',\n",
       " 'beneficiary',\n",
       " 'benefits',\n",
       " 'bennys',\n",
       " 'bergkamp',\n",
       " 'best',\n",
       " 'bestcongrats',\n",
       " 'bestrply',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'beth',\n",
       " 'betta',\n",
       " 'better',\n",
       " 'bettersn',\n",
       " 'beverage',\n",
       " 'bevieswaz',\n",
       " 'beware',\n",
       " 'bf',\n",
       " 'bffs',\n",
       " 'bfore',\n",
       " 'bhaskar',\n",
       " 'bhayandar',\n",
       " 'bian',\n",
       " 'biatch',\n",
       " 'bid',\n",
       " 'bids',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bike',\n",
       " 'billed',\n",
       " 'billing',\n",
       " 'billion',\n",
       " 'bills',\n",
       " 'billy',\n",
       " 'bilo',\n",
       " 'bimbo',\n",
       " 'bin',\n",
       " 'biola',\n",
       " 'biolas',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birla',\n",
       " 'biro',\n",
       " 'birth',\n",
       " 'birthdate',\n",
       " 'birthday',\n",
       " 'bishan',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitching',\n",
       " 'bite',\n",
       " 'bites',\n",
       " 'bits',\n",
       " 'biz',\n",
       " 'bk',\n",
       " 'black',\n",
       " 'blackand',\n",
       " 'blackberry',\n",
       " 'blackim',\n",
       " 'blacko',\n",
       " 'blah',\n",
       " 'blakes',\n",
       " 'blame',\n",
       " 'blank',\n",
       " 'blanked',\n",
       " 'blanket',\n",
       " 'blankets',\n",
       " 'blastin',\n",
       " 'bleak',\n",
       " 'bleh',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessget',\n",
       " 'blessing',\n",
       " 'blessings',\n",
       " 'blimey',\n",
       " 'blind',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blog',\n",
       " 'blogging',\n",
       " 'bloke',\n",
       " 'blokes',\n",
       " 'blonde',\n",
       " 'bloo',\n",
       " 'blood',\n",
       " 'bloodblood',\n",
       " 'bloodsend',\n",
       " 'bloody',\n",
       " 'bloomberg',\n",
       " 'bloombergcom',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blu',\n",
       " 'blue',\n",
       " 'bluetooth',\n",
       " 'bluetoothhdset',\n",
       " 'blueu',\n",
       " 'bluff',\n",
       " 'blur',\n",
       " 'bluray',\n",
       " 'bmw',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'boatin',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'boggy',\n",
       " 'bognor',\n",
       " 'bold',\n",
       " 'bollox',\n",
       " 'boltblue',\n",
       " 'bomb',\n",
       " 'bone',\n",
       " 'bong',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'boobs',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'bookedthe',\n",
       " 'booking',\n",
       " 'bookmark',\n",
       " 'books',\n",
       " 'bookshelf',\n",
       " 'boooo',\n",
       " 'boost',\n",
       " 'booty',\n",
       " 'bootydelious',\n",
       " 'borderline',\n",
       " 'bored',\n",
       " 'borin',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'bornplease',\n",
       " 'borrow',\n",
       " 'boss',\n",
       " 'boston',\n",
       " 'bot',\n",
       " 'bother',\n",
       " 'bothering',\n",
       " 'bottle',\n",
       " 'bought',\n",
       " 'boundaries',\n",
       " 'bout',\n",
       " 'boutxx',\n",
       " 'bowa',\n",
       " 'bowl',\n",
       " 'bowls',\n",
       " 'box',\n",
       " 'boxcpm',\n",
       " 'boxm',\n",
       " 'boxnqp',\n",
       " 'boxqu',\n",
       " 'boxskch',\n",
       " 'boxskwpppm',\n",
       " 'boxwrc',\n",
       " 'boy',\n",
       " 'boye',\n",
       " 'boyf',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'boytoy',\n",
       " 'boyy',\n",
       " 'bpo',\n",
       " 'brah',\n",
       " 'brain',\n",
       " 'braindance',\n",
       " 'brainless',\n",
       " 'brains',\n",
       " 'brainy',\n",
       " 'brand',\n",
       " 'brandy',\n",
       " 'bras',\n",
       " 'brats',\n",
       " 'braved',\n",
       " 'bray',\n",
       " 'brb',\n",
       " 'brdget',\n",
       " 'bread',\n",
       " 'breadstick',\n",
       " 'break',\n",
       " 'breaker',\n",
       " 'breakfast',\n",
       " 'breakin',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breather',\n",
       " 'breathing',\n",
       " 'breeze',\n",
       " 'breezy',\n",
       " 'brekkie',\n",
       " 'bribe',\n",
       " 'bridge',\n",
       " 'bridgwater',\n",
       " 'brief',\n",
       " 'bright',\n",
       " 'brighten',\n",
       " 'brilliant',\n",
       " 'brilliantly',\n",
       " 'brilliantthingi',\n",
       " 'brin',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brisk',\n",
       " 'brison',\n",
       " 'bristol',\n",
       " 'british',\n",
       " 'britney',\n",
       " 'bro',\n",
       " 'broad',\n",
       " 'broadband',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brolly',\n",
       " 'bros',\n",
       " 'broth',\n",
       " 'brothas',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'browni',\n",
       " 'brownie',\n",
       " 'brownies',\n",
       " 'browse',\n",
       " 'browser',\n",
       " 'browsin',\n",
       " 'bruce',\n",
       " 'brum',\n",
       " 'bruv',\n",
       " 'bslvyl',\n",
       " 'bsn',\n",
       " 'bsnl',\n",
       " 'bstfrnd',\n",
       " 'bt',\n",
       " 'bthere',\n",
       " 'bthmm',\n",
       " 'btnational',\n",
       " 'btnationalrate',\n",
       " 'btooth',\n",
       " 'btw',\n",
       " 'btwn',\n",
       " 'bu',\n",
       " 'bucks',\n",
       " 'bud',\n",
       " 'buddy',\n",
       " 'buddys',\n",
       " 'budget',\n",
       " 'buen',\n",
       " 'buff',\n",
       " 'buffet',\n",
       " 'buffy',\n",
       " 'bugis',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bulbs',\n",
       " 'bull',\n",
       " 'bullshit',\n",
       " 'bunch',\n",
       " 'bundle',\n",
       " 'bunkers',\n",
       " 'buns',\n",
       " 'burden',\n",
       " 'burger',\n",
       " 'burgundy',\n",
       " 'burial',\n",
       " 'burn',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'burnt',\n",
       " 'burrito',\n",
       " 'bus',\n",
       " 'buses',\n",
       " 'busetop',\n",
       " 'business',\n",
       " 'busty',\n",
       " 'busy',\n",
       " 'busyi',\n",
       " 'butt',\n",
       " 'buttheres',\n",
       " 'butting',\n",
       " 'buttons',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'buyers',\n",
       " 'buying',\n",
       " 'buz',\n",
       " 'buzy',\n",
       " 'buzz',\n",
       " 'buzzzz',\n",
       " 'bw',\n",
       " 'bx',\n",
       " 'bxipwe',\n",
       " 'byatch',\n",
       " 'bye',\n",
       " 'cab',\n",
       " 'cabin',\n",
       " 'cable',\n",
       " 'cafe',\n",
       " 'cage',\n",
       " 'cake',\n",
       " 'caken',\n",
       " 'cakes',\n",
       " 'cal',\n",
       " 'calculated',\n",
       " 'calculation',\n",
       " 'cali',\n",
       " 'calicut',\n",
       " 'california',\n",
       " 'calis',\n",
       " 'callback',\n",
       " 'callcost',\n",
       " 'callcoz',\n",
       " 'calld',\n",
       " 'calldrove',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'callers',\n",
       " 'callertune',\n",
       " 'callfreefone',\n",
       " 'callin',\n",
       " 'calling',\n",
       " 'callingforgot',\n",
       " 'callon',\n",
       " 'calloptout',\n",
       " 'calloptoutfq',\n",
       " 'calloptouthf',\n",
       " 'calloptoutj',\n",
       " 'calloptoutjq',\n",
       " 'calloptoutlf',\n",
       " 'calloptoutndx',\n",
       " 'calloptoutqf',\n",
       " 'calloptoutyhl',\n",
       " 'calls',\n",
       " 'callsmessagesmissed',\n",
       " 'callsppm',\n",
       " 'callurgent',\n",
       " 'calm',\n",
       " 'cam',\n",
       " 'camcorder',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'cameravideo',\n",
       " 'camp',\n",
       " 'campus',\n",
       " 'camry',\n",
       " 'canada',\n",
       " 'canal',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:24:49.464096Z",
     "start_time": "2019-03-17T15:24:49.453418Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:24:49.655120Z",
     "start_time": "2019-03-17T15:24:49.648099Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words.extend(['aa','aah','aaniye','abj','ag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:37:17.542730Z",
     "start_time": "2019-03-17T15:37:17.409752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5571x8472 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 45591 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(data['Text'])\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:24:51.467527Z",
     "start_time": "2019-03-17T15:24:51.253946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>aathilove</th>\n",
       "      <th>aathiwhere</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abeg</th>\n",
       "      <th>abelu</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abi</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zs</th>\n",
       "      <th>zyada</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8472 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaooooright  aathilove  aathiwhere   ab  abbey  abdomen  abeg  abelu  \\\n",
       "0          0.0        0.0         0.0  0.0    0.0      0.0   0.0    0.0   \n",
       "1          0.0        0.0         0.0  0.0    0.0      0.0   0.0    0.0   \n",
       "2          0.0        0.0         0.0  0.0    0.0      0.0   0.0    0.0   \n",
       "3          0.0        0.0         0.0  0.0    0.0      0.0   0.0    0.0   \n",
       "4          0.0        0.0         0.0  0.0    0.0      0.0   0.0    0.0   \n",
       "\n",
       "   aberdeen  abi ...   zhong  zindgi  zoe  zogtorius  zoom  zouk   zs  zyada  \\\n",
       "0       0.0  0.0 ...     0.0     0.0  0.0        0.0   0.0   0.0  0.0    0.0   \n",
       "1       0.0  0.0 ...     0.0     0.0  0.0        0.0   0.0   0.0  0.0    0.0   \n",
       "2       0.0  0.0 ...     0.0     0.0  0.0        0.0   0.0   0.0  0.0    0.0   \n",
       "3       0.0  0.0 ...     0.0     0.0  0.0        0.0   0.0   0.0  0.0    0.0   \n",
       "4       0.0  0.0 ...     0.0     0.0  0.0        0.0   0.0   0.0  0.0    0.0   \n",
       "\n",
       "   üll  〨ud  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "\n",
       "[5 rows x 8472 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets consider TF-idf\n",
    "Dense_mat = X_train_tfidf.todense()\n",
    "A = pd.DataFrame(Dense_mat, columns=tfidf_transformer.get_feature_names())\n",
    "A.shape\n",
    "A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:26:39.175090Z",
     "start_time": "2019-03-17T15:26:39.167155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5571, 8472)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform the train test split on the data and then build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:35:51.759266Z",
     "start_time": "2019-03-17T15:35:51.732865Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:35:54.324766Z",
     "start_time": "2019-03-17T15:35:52.361375Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(Dense_mat, data['Type'],test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:35:54.811420Z",
     "start_time": "2019-03-17T15:35:54.800086Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3899, 8472)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:35:55.259560Z",
     "start_time": "2019-03-17T15:35:55.248059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 8472)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:35:55.713792Z",
     "start_time": "2019-03-17T15:35:55.701564Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3899,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:35:56.130649Z",
     "start_time": "2019-03-17T15:35:56.108079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3378\n",
       "spam     521\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:35:56.566557Z",
     "start_time": "2019-03-17T15:35:56.547648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     1446\n",
       "spam     226\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T04:33:53.970330Z",
     "start_time": "2019-03-18T04:33:53.120057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required Libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Building the Naive Bayes Model\n",
    "clf_train = MultinomialNB()\n",
    "\n",
    "# Modelling Up on Train & Predicting up on Test\n",
    "clf_train.fit(X_train, y_train)\n",
    "pred = clf_train.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T04:33:54.348558Z",
     "start_time": "2019-03-18T04:33:54.331518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1446,    0],\n",
       "       [  67,  159]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = confusion_matrix(y_test,pred)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T04:33:54.589235Z",
     "start_time": "2019-03-18T04:33:54.566050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9599282296650717\n",
      "0.7035398230088495\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "rec = recall_score(y_test, pred,pos_label='spam')\n",
    "prec = precision_score(y_test, pred,pos_label='spam')\n",
    "\n",
    "# Printing the Metrics\n",
    "print(acc)\n",
    "print(rec)\n",
    "print(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implmenting lositic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T04:33:54.721818Z",
     "start_time": "2019-03-18T04:33:54.591703Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samyam/anaconda3/envs/insofe/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing the Logistic-Regression Model from SKLearn Package\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Building the Logistic Regression Model\n",
    "logreg_train = LogisticRegression()\n",
    "clf_lr = logreg_train.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T04:33:54.760339Z",
     "start_time": "2019-03-18T04:33:54.725046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9521531100478469\n",
      "0.6592920353982301\n",
      "0.9802631578947368\n"
     ]
    }
   ],
   "source": [
    "# Test Metrics\n",
    "pred_lr_test = clf_lr.predict(X_test)\n",
    "acc_lr_test = accuracy_score(y_test, pred_lr_test)\n",
    "rec_lr_test = recall_score(y_test, pred_lr_test, pos_label='spam')\n",
    "prec_lr_test = precision_score(y_test, pred_lr_test, pos_label='spam')\n",
    "\n",
    "# Printing the Metrics\n",
    "print(acc_lr_test)\n",
    "print(rec_lr_test)\n",
    "print(prec_lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1410px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
