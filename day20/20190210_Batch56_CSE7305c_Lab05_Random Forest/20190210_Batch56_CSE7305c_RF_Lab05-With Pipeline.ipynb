{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Description:\n",
    "* A relatively young bank is growing rapidly in terms of overall customer acquisition.Majority of these are Liability customers with varying sizes of relationship with the bank.The customer base of Asset customers is quite small, and the bank WANTS to grow this base rapidly to bring in more loan business. \n",
    "\n",
    "* Specifically, it wants to explore ways of converting its liability customers to Personal Loan customers.\n",
    "\n",
    "* A campaign the bank ran for liability customers last year showed a healthy conversion rate of over 9% successes. This has encouraged the Retail Marketing department to devise smarter campaigns with better target marketing.\n",
    "\n",
    "#### Anlaytics Objectives :\n",
    "\t\n",
    "\t\n",
    "* 1\tWhile designing a new campaign, can we model the previous campaign's customer behavior to \n",
    "\tanalyze what combination of parameters make a customer more likely to \n",
    "\taccept a personal loan?\n",
    "\t\n",
    "* 2\tThere are several special products / facilities the bank offers like CD and security accounts, \n",
    "\tonline services, credit cards, etc. Can we spot any association among these\n",
    "\tfor finding cross-selling opportunities?\n",
    "\n",
    "#### Data Set Description :\n",
    "* ID:\tCustomer ID\t\t\t\n",
    "* Age:\tCustomer's age in completed years\t\t\t\n",
    "* Experience:\t#years of professional experience\t\t\t\n",
    "* Income:\tAnnual income of the customer in thousands of Dollars\t\t\t\n",
    "* ZIPCode:\tHome Address ZIP code.\t\t\tDo not use ZIP code\n",
    "* Family:\tFamily size of the customer\t\t\t\n",
    "* CCAvg:\tAvg. spending on credit cards per month in thousands of Dollars\t\t\n",
    "* Education:\tEducation Level. 1: Undergrad; 2: Graduate; 3: Advanced/Professional\t\t\t\n",
    "* Mortgage:\tValue of house mortgage if any. (thousands of Dollars)\t\t\t\n",
    "* **Personal Loan:\tDid this customer accept the personal loan offered in the last campaign?**\t\t\t\n",
    "* Securities Account:\tDoes the customer have a securities account with the bank?\t\t\t\n",
    "* CD Account:\tDoes the customer have a certificate of deposit (CD) account with the bank?\t\t\t\n",
    "* Online:\tDoes the customer use internet banking facilities?\t\t\t\n",
    "* CreditCard:\tDoes the customer use a credit card issued by UniversalBank?\t\t\t\n",
    "\n",
    "#### Note:\n",
    "* While reading the data set  replace the '?',',' as NAs\n",
    "\n",
    "#### Experiment :\n",
    "* Building a Random Forest to predict whether a person takes personal loan or not\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:28.244Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "path = os.getcwd()\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:28.252Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Check the dimensions and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:28.415Z"
    }
   },
   "outputs": [],
   "source": [
    "bank=pd.read_csv(\"UniversalBank.csv\",na_values=[\"?\",\",\"])\n",
    "print(\"The number of Rows in the bank data set  =\"+str(bank.shape[0]))\n",
    "print(\"The number of Columns in the bank data set =\" +str(bank.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Columns names and check the datatypes of columns(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:28.556Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The columns in the data set are : \\n\",list(bank.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:28.559Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The data types of the columns are :\\n \",bank.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the top 10 rows to glance the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:28.846Z"
    }
   },
   "outputs": [],
   "source": [
    "bank.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:28.858Z"
    }
   },
   "outputs": [],
   "source": [
    "bank.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the summary of dataframe(describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.038Z"
    }
   },
   "outputs": [],
   "source": [
    "bank.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the unique levels in the target attribute Personal and also check for the percentage distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.167Z"
    }
   },
   "outputs": [],
   "source": [
    "bank[\"Personal Loan\"].value_counts()/bank.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the number of unique ZIP Codes present in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.305Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The number of Unique ZIP Codes in the bank data set\",bank['ZIP Code'].value_counts().count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Unique counts of  Family members  present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.433Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The number of Unique Family members in the bank data set: \\n\",bank['Family'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Unique counts of Education levels present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.562Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The number of Unique Education levels in the bank data set: \\n\",bank['Education'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do Necessary changes for the data types from the previous observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.694Z"
    }
   },
   "outputs": [],
   "source": [
    "bank['Education']=bank['Education'].astype('category')\n",
    "bank['CD Account']=bank['CD Account'].astype('category')\n",
    "bank['Online']=bank['Online'].astype('category')\n",
    "bank['CreditCard']=bank['CreditCard'].astype('category')\n",
    "bank['Securities Account']=bank['Securities Account'].astype('category')\n",
    "bank['Family']=bank['Family'].astype('category')\n",
    "bank['ZIP Code']=bank['ZIP Code'].astype('category')\n",
    "######################################################################################################################\n",
    "#Use the following code when you have more aolumns\n",
    "# for column in ['Education', 'CD Account', 'Online']:\n",
    "#     bank[column]=bank[column].astype('category')\n",
    "#bank[bank.select_dtypes(['object']).columns] = bank.select_dtypes(['object']).apply(lambda x: x.astype('category')) \n",
    "#bank[bank.select_dtypes(['object']).columns] = bank.select_dtypes(['object']).apply(lambda x: x.astype('category')) \n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.697Z"
    }
   },
   "outputs": [],
   "source": [
    "bank.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the unncessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.825Z"
    }
   },
   "outputs": [],
   "source": [
    "bank=bank.drop([\"ID\",\"ZIP Code\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.829Z"
    }
   },
   "outputs": [],
   "source": [
    "bank.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Check the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:29.952Z"
    }
   },
   "outputs": [],
   "source": [
    "bank.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPLIT THE data in to train and test(use sklearn package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.077Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.080Z"
    }
   },
   "outputs": [],
   "source": [
    "y=bank[\"Personal Loan\"]\n",
    "X=bank.drop('Personal Loan', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.084Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the numerical and Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.210Z"
    }
   },
   "outputs": [],
   "source": [
    "num_attr = X.select_dtypes(include=['float64','int64']).columns\n",
    "num_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.213Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_attr = X.select_dtypes(exclude=['float64','int64']).columns\n",
    "cat_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Standardize the data (numerical attributes only)( import StandardScaler) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.337Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.340Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(memory ='./' ,steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.343Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(memory = './',steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.346Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_attr),\n",
    "        ('cat', categorical_transformer, cat_attr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now It is time for Model BUilding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us see the details of the Random Forest in Sklearn \n",
    "* class sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "\n",
    "\n",
    "* A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "* **n_estimators** : integer, optional (default=10)\n",
    "    **The number of trees in the forest**.The default value of n_estimators will change from 10 in version 0.20 to 100 in version 0.22.\n",
    "\n",
    "* **criterion** : string, optional (default=”gini”)\n",
    "    The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.\n",
    "\n",
    "* **max_depth** : integer or None, optional (default=None)\n",
    "    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "* **min_samples_split** : int, float, optional (default=2)\n",
    "    The minimum number of samples required to split an internal node:\n",
    "\n",
    "    If int, then consider min_samples_split as the minimum number.\n",
    "    If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "\n",
    "* **min_samples_leaf** : int, float, optional (default=1)\n",
    "    The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "    If int, then consider min_samples_leaf as the minimum number.\n",
    "    If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "\n",
    "* **min_weight_fraction_leaf** : float, optional (default=0.)\n",
    "    The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "* **max_features** : int, float, string or None, optional (default=”auto”)\n",
    "    The number of features to consider when looking for the best split:\n",
    "\n",
    "    If int, then consider max_features features at each split.\n",
    "    If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "    If “auto”, then max_features=sqrt(n_features).\n",
    "    If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "    If “log2”, then max_features=log2(n_features).\n",
    "    If None, then max_features=n_features.\n",
    "    Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n",
    "\n",
    "* **max_leaf_nodes** : int or None, optional (default=None)\n",
    "    Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "* **min_impurity_decrease** : float, optional (default=0.)\n",
    "    A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "  The weighted impurity decrease equation is the following:\n",
    "\n",
    "    N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                    - N_t_L / N_t * left_impurity)\n",
    "    where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.\n",
    "\n",
    "    N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.\n",
    "\n",
    "* **bootstrap** : boolean, optional (default=True)\n",
    "    Whether bootstrap samples are used when building trees.\n",
    "\n",
    "* **oob_score** : bool (default=False)\n",
    "    Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "\n",
    "* **n_jobs** : int or None, optional (default=None)\n",
    "    The number of jobs to run in parallel for both fit and predict. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
    "\n",
    "* **random_state** : int, RandomState instance or None, optional (default=None)\n",
    "    If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "* **verbose** : int, optional (default=0)\n",
    "    Controls the verbosity when fitting and predicting.\n",
    "\n",
    "* **warm_start** : bool, optional (default=False)\n",
    "    When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See the Glossary.\n",
    "\n",
    "* **class_weight** : dict, list of dicts, “balanced”, “balanced_subsample” or None, optional (default=None)\n",
    "    Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "\n",
    "    Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].\n",
    "\n",
    "    The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "    The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.\n",
    "\n",
    "    For multi-output, the weights of each column of y will be multiplied.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Build Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:30.868Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=10,max_depth=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Append classifier to preprocessing pipeline.Now we have a full prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.000Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = Pipeline(memory = './',steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.003Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "print(\"Train Accuracy = \",accuracy_score(y_train,y_pred))\n",
    "print(\"Recall in train = \",recall_score(y_train,y_pred,pos_label=1))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Accuracy = \",accuracy_score(y_test,y_pred))\n",
    "print(\"Recall on Test = \",recall_score(y_test,y_pred,pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.007Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.named_steps['classifier'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.010Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -la ./joblib/sklearn/pipeline/_fit_transform_one/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.014Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "features =clf.steps[1][1].columns\n",
    "print(features)\n",
    "importances = clf.named_steps['classifier'].feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "print(indices)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "print([features[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.180Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "rfc = RandomForestClassifier(n_jobs=-1, max_features='sqrt') \n",
    " \n",
    "# Use a grid over parameters of interest\n",
    "param_grid = { \n",
    "           \"n_estimators\" : [9, 18, 27, 36, 45, 54, 63],\n",
    "           \"max_depth\" : [2,3,5,7],\n",
    "           \"min_samples_leaf\" : [2, 4]}\n",
    " \n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10)\n",
    "CV_rfc.fit(X=X_train, y=y_train)\n",
    "print (CV_rfc.best_score_, CV_rfc.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.185Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test=CV_rfc.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.188Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_test,y_pred_test, pos_label=1, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.384Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(n_estimators=10)\n",
    "clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.515Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.645Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "print(accuracy_score(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.648Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.651Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "param_grid = {\n",
    "    'base_estimator__max_depth' : [1, 2, 3, 4, 5],\n",
    "    'max_samples' : [0.05, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(),\n",
    "                                     n_estimators = 100, max_features = 0.5),\n",
    "                   param_grid,scoring='recall')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.653Z"
    }
   },
   "outputs": [],
   "source": [
    "print(clf.best_score_,clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-19T07:14:31.656Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import recall_score\n",
    "trainpreds=clf.predict(X_train)\n",
    "print(recall_score(y_train,trainpreds,pos_label=1))# recall\n",
    "print(accuracy_score(y_train,trainpreds))# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1410px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
