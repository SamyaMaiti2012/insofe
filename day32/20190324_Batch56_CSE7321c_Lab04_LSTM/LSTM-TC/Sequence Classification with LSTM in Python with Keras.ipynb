{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence classification\n",
    "\n",
    "Is a predictive modeling problem where you have some sequence of inputs over space or time and the task is to predict a category for the sequence.\n",
    "\n",
    "What makes this problem difficult is that the sequences can vary in length, be comprised of a very large vocabulary of input symbols and may require the model to learn the long-term context or dependencies between symbols in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem Description\n",
    "\n",
    "IMDB movie review sentiment classification problem.\n",
    "\n",
    "    The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. \n",
    "    The problem is to determine whether a given movie review has a positive or negative sentiment.\n",
    "\n",
    "Keras comes with in-built IMDB dataset. The __imdb.load_data()__ function allows you to load the dataset in a format that is ready for use in neural network and deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the classes and functions required "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "\n",
    "from keras.datasets import imdb #A utility to load a dataset\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "from keras.preprocessing import sequence #To convert a variable length sentence into a prespecified length\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the IMDB dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 77s 4us/step\n"
     ]
    }
   ],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's inspect the first one sentences of train and their classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ list([1, 14, 20, 100, 28, 77, 6, 542, 503, 20, 48, 342, 470, 7, 4, 4, 20, 286, 38, 76, 4085, 23, 4, 383, 139, 13, 384, 240, 6, 383, 2, 5, 146, 252, 15, 225, 6, 176, 53, 15, 271, 23, 19, 383, 2, 1005, 7, 260, 383, 23, 6, 1813, 2857, 488, 2, 2, 122, 6, 52, 292, 1069, 51, 32, 29, 69, 8, 81, 63, 286, 76, 33, 31, 213, 42, 160, 31, 62, 28, 8, 462, 33, 90, 88, 27, 109, 16, 38, 2, 2, 2, 16, 2659, 11, 41, 217, 17, 4, 1947, 383, 2, 59, 2856, 7, 224, 53, 151, 5, 146, 24, 2, 41, 260, 383, 4, 415, 15, 3405, 46, 4, 91, 8, 72, 11, 14, 20, 16, 2, 2, 11, 41, 1078, 217, 17, 4, 1715, 5, 1947, 322, 225, 142, 44, 307, 1004, 5, 46, 15, 2303, 2, 8, 72, 59, 256, 15, 217, 5, 17, 25, 296, 4, 20, 25, 380, 8, 235, 78, 18, 41, 10, 10, 2, 7, 6, 383, 2, 137, 24, 735, 819, 42, 6, 682, 356, 8, 2, 1553, 9, 179, 2, 5, 127, 6, 1257, 292, 11, 800, 25, 89, 2066, 965, 2624, 70, 193, 120, 5, 2457, 4, 55, 183, 11, 113, 25, 104, 545, 7])]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:1])\n",
    "print(y_train[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate and pad the input sequences so that they are all the same length for modeling. \n",
    "\n",
    "The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the same sentences after padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    1   14   20  100   28   77    6  542\n",
      "   503   20   48  342  470    7    4    4   20  286   38   76 4085   23\n",
      "     4  383  139   13  384  240    6  383    2    5  146  252   15  225\n",
      "     6  176   53   15  271   23   19  383    2 1005    7  260  383   23\n",
      "     6 1813 2857  488    2    2  122    6   52  292 1069   51   32   29\n",
      "    69    8   81   63  286   76   33   31  213   42  160   31   62   28\n",
      "     8  462   33   90   88   27  109   16   38    2    2    2   16 2659\n",
      "    11   41  217   17    4 1947  383    2   59 2856    7  224   53  151\n",
      "     5  146   24    2   41  260  383    4  415   15 3405   46    4   91\n",
      "     8   72   11   14   20   16    2    2   11   41 1078  217   17    4\n",
      "  1715    5 1947  322  225  142   44  307 1004    5   46   15 2303    2\n",
      "     8   72   59  256   15  217    5   17   25  296    4   20   25  380\n",
      "     8  235   78   18   41   10   10    2    7    6  383    2  137   24\n",
      "   735  819   42    6  682  356    8    2 1553    9  179    2    5  127\n",
      "     6 1257  292   11  800   25   89 2066  965 2624   70  193  120    5\n",
      "  2457    4   55  183   11  113   25  104  545    7]] [0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:1], y_train[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is the Embedded layer that uses 32 length vectors to represent each word. \n",
    "\n",
    "The next layer is the LSTM layer with 100 memory units (smart neurons). \n",
    "\n",
    "Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model lstm\n",
    "embedding_vector_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is a binary classification problem, log loss is used as the loss function (binary_crossentropy in Keras). \n",
    "\n",
    "The efficient ADAM optimization algorithm is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit for only 2 epochs because it quickly overfits the problem. \n",
    "\n",
    "A large batch size of 64 reviews is used to space out weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 380s 15ms/step - loss: 0.5240 - acc: 0.7296 - val_loss: 0.3914 - val_acc: 0.8291\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 351s 14ms/step - loss: 0.3063 - acc: 0.8752 - val_loss: 0.2945 - val_acc: 0.8786\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 353s 14ms/step - loss: 0.2639 - acc: 0.8932 - val_loss: 0.3056 - val_acc: 0.8740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f31af5a1e48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we estimate the performance of the model on unseen reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.40%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSTM For Sequence Classification With Dropout\n",
    "\n",
    "LSTM generally have the problem of overfitting.\n",
    "\n",
    "Dropout can be applied between layers using the Dropout Keras layer. \n",
    "\n",
    "    We can do this easily by adding new Dropout layers between the Embedding and LSTM layers and the LSTM and Dense output layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 320s 13ms/step - loss: 0.4617 - acc: 0.7700\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 286s 11ms/step - loss: 0.3674 - acc: 0.8448\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 263s 11ms/step - loss: 0.2688 - acc: 0.8934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f31a401b320>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.30%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: \n",
    "    \n",
    "    Dropout having the desired impact on training with a slightly slower trend in convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "\n",
    "    https://machinelearningmastery.com\n",
    "\n",
    "    https://keras.io"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
