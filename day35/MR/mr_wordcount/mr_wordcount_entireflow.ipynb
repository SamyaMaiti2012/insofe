{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted mr_wordcount_input\r\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -rm -r -skipTrash mr_wordcount_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted mr_wordcount_output\r\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -rm -r -skipTrash mr_wordcount_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir mr_wordcount_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A fat cat sat on the mat\"\r\n",
      "\"Hello lets learn Mapreduce\"\r\n",
      "\"A fat cat sat on the mat\"\r\n",
      "\"Hello lets learn something interesting\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal input.txt mr_wordcount_input/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "import sys # inbuilt module, here we use to take input from the console     \r\n",
      "for line in sys.stdin: # start a for loop, this is\r\n",
      "# an infinite loop which runs as long as there is input, \r\n",
      "#sys.stdin will take input from the console\r\n",
      "    line = line.strip() #.strip() function will remove the leading and trailing spaces, if any.\r\n",
      "    keys = line.split() # .split() function will split the input based on the string passed as argument,\r\n",
      "#\tif nothing is passed as an argument, space is taken as default\r\n",
      "#initiating another for loop to print the words and 1\r\n",
      "    for key in keys:     \r\n",
      "\t\tvalue = 1\r\n",
      "\t\tprint('{0}\\t{1}'.format(key, value)) \r\n",
      "# this will print the word and 1 separated by tab to the console, which the reduces reads as input.\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\"\"\"reducer.py\"\"\"\r\n",
      "\r\n",
      "# from operator import itemgetter\r\n",
      "import sys\r\n",
      "\r\n",
      "current_word = None\r\n",
      "current_count = 0\r\n",
      "word = None\r\n",
      "\r\n",
      "# input comes from STDIN\r\n",
      "for line in sys.stdin:\r\n",
      "    # remove leading and trailing whitespace\r\n",
      "    line = line.strip()\r\n",
      "\r\n",
      "    # parse the input we got from mapper.py\r\n",
      "    word, count = line.split('\\t', 1)\r\n",
      "\r\n",
      "    # convert count (currently a string) to int\r\n",
      "    try:\r\n",
      "        count = int(count)\r\n",
      "    except ValueError:\r\n",
      "        # count was not a number, so silently\r\n",
      "        # ignore/discard this line\r\n",
      "        continue\r\n",
      "\r\n",
      "    # this IF-switch only works because Hadoop sorts map output\r\n",
      "    # by key (here: word) before it is passed to the reducer\r\n",
      "    if current_word == word:\r\n",
      "        current_count += count\r\n",
      "    else:\r\n",
      "        if current_word:\r\n",
      "            # write result to STDOUT\r\n",
      "            print '%s\\t%s' % (current_word, current_count)\r\n",
      "        current_count = count\r\n",
      "        current_word = word\r\n",
      "\r\n",
      "# do not forget to output the last word if needed!\r\n",
      "if current_word == word:\r\n",
      "    print '%s\\t%s' % (current_word, current_count)\r\n"
     ]
    }
   ],
   "source": [
    "!cat reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/hdp/2.6.5.0-292/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.5.0-292.jar] /tmp/streamjob4014563151978163325.jar tmpDir=null\n",
      "19/04/13 11:57:56 INFO client.RMProxy: Connecting to ResourceManager at b.insofe.edu.in/10.10.10.8:8050\n",
      "19/04/13 11:57:56 INFO client.AHSProxy: Connecting to Application History server at b.insofe.edu.in/10.10.10.8:10200\n",
      "19/04/13 11:57:56 INFO client.RMProxy: Connecting to ResourceManager at b.insofe.edu.in/10.10.10.8:8050\n",
      "19/04/13 11:57:56 INFO client.AHSProxy: Connecting to Application History server at b.insofe.edu.in/10.10.10.8:10200\n",
      "19/04/13 11:57:57 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/04/13 11:57:57 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/04/13 11:57:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1552288665821_2265\n",
      "19/04/13 11:57:59 INFO impl.YarnClientImpl: Submitted application application_1552288665821_2265\n",
      "19/04/13 11:57:59 INFO mapreduce.Job: The url to track the job: http://b.insofe.edu.in:8088/proxy/application_1552288665821_2265/\n",
      "19/04/13 11:57:59 INFO mapreduce.Job: Running job: job_1552288665821_2265\n",
      "19/04/13 11:58:12 INFO mapreduce.Job: Job job_1552288665821_2265 running in uber mode : false\n",
      "19/04/13 11:58:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/04/13 11:58:26 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/04/13 11:58:32 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/04/13 11:58:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/04/13 11:58:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/04/13 11:58:48 INFO mapreduce.Job: Job job_1552288665821_2265 completed successfully\n",
      "19/04/13 11:58:48 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=222\n",
      "\t\tFILE: Number of bytes written=786122\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=742\n",
      "\t\tHDFS: Number of bytes written=105\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=215028\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22332\n",
      "\t\tTotal time spent by all map tasks (ms)=71676\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7444\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=71676\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7444\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=660566016\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=68603904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=23\n",
      "\t\tMap output bytes=170\n",
      "\t\tMap output materialized bytes=240\n",
      "\t\tInput split bytes=432\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=13\n",
      "\t\tReduce shuffle bytes=240\n",
      "\t\tReduce input records=23\n",
      "\t\tReduce output records=13\n",
      "\t\tSpilled Records=46\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=558\n",
      "\t\tCPU time spent (ms)=34960\n",
      "\t\tPhysical memory (bytes) snapshot=9835130880\n",
      "\t\tVirtual memory (bytes) snapshot=49745690624\n",
      "\t\tTotal committed heap usage (bytes)=9983492096\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=310\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=105\n",
      "19/04/13 11:58:48 INFO streaming.StreamJob: Output directory: mr_wordcount_output/\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/hdp/2.6.5.0-292/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.reduce.slowstart.completedmaps=1.00 \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files /home/thomasj/Batch56/MR/mr_wordcount/mapper.py,/home/thomasj/Batch56/MR/mr_wordcount/reducer.py \\\n",
    "-input mr_wordcount_input/  \\\n",
    "-output mr_wordcount_output/ \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A\t2\r\n",
      "\"Hello\t2\r\n",
      "Mapreduce\"\t1\r\n",
      "cat\t2\r\n",
      "fat\t2\r\n",
      "interesting\"\t1\r\n",
      "learn\t2\r\n",
      "lets\t2\r\n",
      "mat\"\t2\r\n",
      "on\t2\r\n",
      "sat\t2\r\n",
      "something\t1\r\n",
      "the\t2\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat mr_wordcount_output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted mr_wordcount_input/input.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r -skipTrash mr_wordcount_input/*\n",
    "!hdfs dfs -copyFromLocal Data.txt mr_wordcount_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4 K  mr_wordcount_input/Data.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h mr_wordcount_input/Data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted mr_wordcount_output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r -skipTrash mr_wordcount_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/hdp/2.6.5.0-292/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.5.0-292.jar] /tmp/streamjob2206830520322385958.jar tmpDir=null\n",
      "19/04/13 11:59:07 INFO client.RMProxy: Connecting to ResourceManager at b.insofe.edu.in/10.10.10.8:8050\n",
      "19/04/13 11:59:08 INFO client.AHSProxy: Connecting to Application History server at b.insofe.edu.in/10.10.10.8:10200\n",
      "19/04/13 11:59:08 INFO client.RMProxy: Connecting to ResourceManager at b.insofe.edu.in/10.10.10.8:8050\n",
      "19/04/13 11:59:08 INFO client.AHSProxy: Connecting to Application History server at b.insofe.edu.in/10.10.10.8:10200\n",
      "19/04/13 11:59:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/04/13 11:59:10 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/04/13 11:59:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1552288665821_2266\n",
      "19/04/13 11:59:11 INFO impl.YarnClientImpl: Submitted application application_1552288665821_2266\n",
      "19/04/13 11:59:11 INFO mapreduce.Job: The url to track the job: http://b.insofe.edu.in:8088/proxy/application_1552288665821_2266/\n",
      "19/04/13 11:59:11 INFO mapreduce.Job: Running job: job_1552288665821_2266\n",
      "19/04/13 11:59:23 INFO mapreduce.Job: Job job_1552288665821_2266 running in uber mode : false\n",
      "19/04/13 11:59:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/04/13 11:59:27 INFO mapreduce.Job: Task Id : attempt_1552288665821_2266_m_000001_0, Status : FAILED\n",
      "Rename cannot overwrite non empty destination directory /nfsroot/data/hadoop/yarn/local/usercache/thomasj/appcache/application_1552288665821_2266/filecache/12\n",
      "java.io.IOException: Rename cannot overwrite non empty destination directory /nfsroot/data/hadoop/yarn/local/usercache/thomasj/appcache/application_1552288665821_2266/filecache/12\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:736)\n",
      "\tat org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:237)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:679)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:960)\n",
      "\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:366)\n",
      "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:228)\n",
      "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:221)\n",
      "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:209)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "\n",
      "19/04/13 11:59:31 INFO mapreduce.Job: Task Id : attempt_1552288665821_2266_m_000003_0, Status : FAILED\n",
      "Error: java.io.FileNotFoundException: /nfsroot/data/hadoop/yarn/local/usercache/thomasj/appcache/application_1552288665821_2266/container_e17_1552288665821_2266_01_000004/job.xml (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:235)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:309)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:341)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:400)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:463)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:931)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:912)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:808)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:797)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:596)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.writeLocalJobFile(YarnChild.java:346)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:328)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:148)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143. \n",
      "\n",
      "19/04/13 11:59:34 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/04/13 11:59:38 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/04/13 11:59:41 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/04/13 11:59:45 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/04/13 11:59:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/04/13 11:59:49 INFO mapreduce.Job: Job job_1552288665821_2266 completed successfully\n",
      "19/04/13 11:59:49 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5482\n",
      "\t\tFILE: Number of bytes written=796642\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9142\n",
      "\t\tHDFS: Number of bytes written=2797\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=2\n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=5\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=151257\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=25293\n",
      "\t\tTotal time spent by all map tasks (ms)=50419\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8431\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=50419\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8431\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=464661504\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=77700096\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=44\n",
      "\t\tMap output records=500\n",
      "\t\tMap output bytes=4476\n",
      "\t\tMap output materialized bytes=5500\n",
      "\t\tInput split bytes=428\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=281\n",
      "\t\tReduce shuffle bytes=5500\n",
      "\t\tReduce input records=500\n",
      "\t\tReduce output records=281\n",
      "\t\tSpilled Records=1000\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=433\n",
      "\t\tCPU time spent (ms)=14470\n",
      "\t\tPhysical memory (bytes) snapshot=10007183360\n",
      "\t\tVirtual memory (bytes) snapshot=49733824512\n",
      "\t\tTotal committed heap usage (bytes)=10083631104\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8714\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2797\n",
      "19/04/13 11:59:49 INFO streaming.StreamJob: Output directory: mr_wordcount_output/\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/hdp/2.6.5.0-292/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.reduce.slowstart.completedmaps=0.01 \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-files /home/thomasj/Batch56/MR/mr_wordcount/mapper.py,/home/thomasj/Batch56/MR/mr_wordcount/reducer.py \\\n",
    "-input mr_wordcount_input/  \\\n",
    "-output mr_wordcount_output/ \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"16\t1\r\n",
      "\"Top\t2\r\n",
      "&\t5\r\n",
      "(CMU).\t1\r\n",
      "(Detroit,\t1\r\n",
      "(INSOFE)\t1\r\n",
      "(LTI)\t1\r\n",
      "/\t1\r\n",
      "1\t1\r\n",
      "12\t1\r\n",
      "17.4148Â°N\t2\r\n",
      "2\t1\r\n",
      "2000\t1\r\n",
      "2011\t1\r\n",
      "2011.\t1\r\n",
      "2012,\t1\r\n",
      "2013-2016.[2]\t1\r\n",
      "2014.[5]\t1\r\n",
      "2016\".[3]\t1\r\n",
      "2016\".[4]\t1\r\n",
      "2016.\t1\r\n",
      "3\t1\r\n",
      "3rd\t1\r\n",
      "4\t1\r\n",
      "5\t2\r\n",
      "6\t1\r\n",
      "7\t1\r\n",
      "78.4017Â°E\t1\r\n",
      "78.4017Â°ECoordinates:\t1\r\n",
      "8\t1\r\n",
      "9\t1\r\n",
      "A\t1\r\n",
      "A.I.\t1\r\n",
      "Abercrombie\t1\r\n",
      "Analytics\t4\r\n",
      "Analytics,\t1\r\n",
      "Applied\t2\r\n",
      "Assessment.\t1\r\n",
      "Bengaluru\t3\r\n",
      "Big\t4\r\n",
      "Bradstreet\t1\r\n",
      "CIO.com\t1\r\n",
      "CPEE\t1\r\n",
      "Carnegie\t2\r\n",
      "Certificates\t1\r\n",
      "Certifications\t1\r\n",
      "Collaborations\t2\r\n",
      "Content,\t1\r\n",
      "Contents\t1\r\n",
      "Credila\t1\r\n",
      "D\t1\r\n",
      "Dakshinamurthy\t2\r\n",
      "Dakshinamurthy,\t1\r\n",
      "Data\t7\r\n",
      "Dr.\t5\r\n",
      "Dunn\t1\r\n",
      "Education\t2\r\n",
      "Engineering\t5\r\n",
      "Engineering,\t1\r\n",
      "Established\t1\r\n",
      "Fitch,\t1\r\n",
      "From\t1\r\n",
      "Ganapathi\t1\r\n",
      "HDFC\t1\r\n",
      "History\t2\r\n",
      "Hyderabad\t2\r\n",
      "Hyderabad,\t1\r\n",
      "INSOFE\t11\r\n",
      "INSOFE's\t2\r\n",
      "INSOFEs\t2\r\n",
      "INTUCEO,\t1\r\n",
      "In\t3\r\n",
      "India\t4\r\n",
      "India,\t1\r\n",
      "Initially\t1\r\n",
      "Institute\t2\r\n",
      "Institutes\t2\r\n",
      "International\t4\r\n",
      "Internships\t2\r\n",
      "It\t3\r\n",
      "Jump\t1\r\n",
      "KDnuggets\t1\r\n",
      "Karnataka.\t1\r\n",
      "Kolluru\t1\r\n",
      "Kolluru,\t1\r\n",
      "Kumar\t1\r\n",
      "L\t1\r\n",
      "Language\t2\r\n",
      "Loan\t2\r\n",
      "Location\t1\r\n",
      "Logo.png\t1\r\n",
      "Magazine\t1\r\n",
      "Magazine,\t1\r\n",
      "Meals,\t1\r\n",
      "Mellon\t2\r\n",
      "Michigan\t1\r\n",
      "Mining,\t1\r\n",
      "MoU\t1\r\n",
      "Murthy.\t1\r\n",
      "Of\t1\r\n",
      "Off\"\t1\r\n",
      "Pappu\t1\r\n",
      "Pay\t1\r\n",
      "Pedagogy\t1\r\n",
      "Placement\t2\r\n",
      "Preferred\t1\r\n",
      "President\t1\r\n",
      "Program\t3\r\n",
      "R\t1\r\n",
      "Rankings\t2\r\n",
      "References\t1\r\n",
      "S\t1\r\n",
      "Scholarships\t2\r\n",
      "School\t4\r\n",
      "Science\t1\r\n",
      "Services\t2\r\n",
      "Silicon\t1\r\n",
      "Soothsayer\t1\r\n",
      "Sreerama\t1\r\n",
      "Sridhar\t2\r\n",
      "Steels,\t1\r\n",
      "Students\t1\r\n",
      "Technologies\t2\r\n",
      "Telangana,\t2\r\n",
      "That\t1\r\n",
      "The\t3\r\n",
      "They\t1\r\n",
      "Together\t1\r\n",
      "Training\t2\r\n",
      "USA\t1\r\n",
      "University\t1\r\n",
      "University,\t1\r\n",
      "V\t1\r\n",
      "V.\t1\r\n",
      "Website\t1\r\n",
      "Wikipedia,\t1\r\n",
      "Will\t1\r\n",
      "Within\t1\r\n",
      "Worthington\t1\r\n",
      "a\t2\r\n",
      "academic\t1\r\n",
      "across\t1\r\n",
      "additional\t1\r\n",
      "also\t1\r\n",
      "an\t4\r\n",
      "analytics\t2\r\n",
      "analytics.\t1\r\n",
      "analytics.[1]\t1\r\n",
      "and\t18\r\n",
      "are\t2\r\n",
      "area\t1\r\n",
      "aspects\t1\r\n",
      "assessment\t1\r\n",
      "assists\t1\r\n",
      "at\t1\r\n",
      "avail\t1\r\n",
      "bank\t2\r\n",
      "banks\t1\r\n",
      "based\t4\r\n",
      "both\t2\r\n",
      "build\t1\r\n",
      "business\t2\r\n",
      "by\t2\r\n",
      "campus\t1\r\n",
      "can\t2\r\n",
      "center\t1\r\n",
      "certified\t2\r\n",
      "classroom\t1\r\n",
      "cohort\t1\r\n",
      "collaboration\t2\r\n",
      "commenced\t1\r\n",
      "company)\t1\r\n",
      "consecutively\t1\r\n",
      "consulting\t2\r\n",
      "content,\t1\r\n",
      "corporate\t1\r\n",
      "criteria.\t1\r\n",
      "data\t4\r\n",
      "delivered\t2\r\n",
      "department\t1\r\n",
      "developed\t1\r\n",
      "drives\t1\r\n",
      "during\t1\r\n",
      "early-2016.\t1\r\n",
      "education\t1\r\n",
      "eligibility\t1\r\n",
      "eligible\t1\r\n",
      "encyclopedia\t1\r\n",
      "engineering\t1\r\n",
      "entrance\t1\r\n",
      "etc.\t1\r\n",
      "excellence\t1\r\n",
      "expanded\t1\r\n",
      "extended\t1\r\n",
      "field\t1\r\n",
      "first\t1\r\n",
      "focus\t2\r\n",
      "for\t4\r\n",
      "free\t1\r\n",
      "from\t3\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat mr_wordcount_output/* | head -200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "    https://hadoop.apache.org/docs/r1.2.1/streaming.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
