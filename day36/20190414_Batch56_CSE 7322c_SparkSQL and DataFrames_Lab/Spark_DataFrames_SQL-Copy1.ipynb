{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Operations using Spark DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Agenda\n",
    "-  What are DataFrames in Spark ?\n",
    "-  Different ways to create a DataFrames\n",
    "-  What are Spark Transformations & Actions\n",
    "-  Verify Summary Statistics\n",
    "-  Spark SQL\n",
    "-  Performance Comparison of Spark DataFrame and Spark SQL\n",
    "-  Column References\n",
    "-  Converting to Spark Types - Literals\n",
    "-  Add/Rename/Remove Columns\n",
    "-  TypeCasting\n",
    "-  Column differences\n",
    "-  Pair-wise frequencies\n",
    "-  Remove duplicates\n",
    "-  Working with Nulls\n",
    "-  Filtering the rows\n",
    "-  Aggregations\n",
    "-  Joins\n",
    "-  Random Samples\n",
    "-  Random Splits\n",
    "-  Map Transformations\n",
    "-  Sorting\n",
    "-  Union\n",
    "-  String Manipulations\n",
    "-  Regular Expressions\n",
    "-  Working with Dates and Time Stamp\n",
    "-  User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Set Python - Spark environment.\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/py4j-0.10.4-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] + \"/pyspark.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The entry point to programming Spark with the Dataset and DataFrame API is SparkSession.\n",
    "* A SparkSession can be used to  create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files etc.,\n",
    "* Spark Session is available in pyspark package as a sub package called pyspark.sql module\n",
    "  *  Reference :https://spark.apache.org/docs/2.3.0/api/python/pyspark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Create  SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "spark = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* _builder_ : A class attribute having a Builder method to construct SparkSession instances\n",
    "\n",
    "* _AppName_: Sets a name for the application, which will be shown in the Spark web UI.\n",
    "   \n",
    "* _Config_: Sets a config option. Options set using this method are automatically propagated to both SparkConf and SparkSession‘s own configuration.\n",
    "\n",
    "* _master_:It  is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local modemaster \n",
    "\n",
    "* _getOrCreate()_: Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder.\n",
    "\n",
    "* _enableHiveSupport()_: Enables Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://i.insofe.edu.in:4046\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkDataFrames_and_SQL_on_Sales_Dataset</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27eb1d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ** Spark DataFrame **\n",
    "\n",
    "#### A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. \n",
    "<br> The list that defines the columns and the types within those columns is called the schema. \n",
    "<br> One can think of a DataFrame as a spreadsheet with named columns.\n",
    "<br> A spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span thousands of computers.\n",
    "<br> The reason for putting the data on more than one computer should be intuitive: \n",
    "<br>     either the data is too large to fit on one machine or \n",
    "<br>     it would simply take too long to perform that computation on one machine.\n",
    "\n",
    "#### NOTE\n",
    "Spark has several core abstractions: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets (RDDs). \n",
    "<br> These different abstractions all represent distributed collections of data. \n",
    "<br> The easiest and most efficient are DataFrames, which are available in all languages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create a dataframe with one column containing 100 rows with values from 0 to 99.\n",
    "This range of numbers represents a distributed collection. \n",
    "<br> When run on a cluster, each part of this range of numbers exists on a different executor. \n",
    "<br> This is a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "myRange = spark.range(100).toDF('number')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check myRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRange.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "myDF = spark.__________([[1, 'Alice', 30], [2, 'Bob', 28], [3, 'Cathy', 31], [4, 'Denice', 25]], ['Id', 'Name', 'Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| Id|  Name|Age|\n",
      "+---+------+---+\n",
      "|  1| Alice| 30|\n",
      "|  2|   Bob| 28|\n",
      "|  3| Cathy| 31|\n",
      "|  4|Denice| 25|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important classes of pyspark.sql\n",
    "* pyspark.sql.SparkSession :            Main entry point for DataFrame and SQL functionality.\n",
    "* pyspark.sql.DataFrame    :    A distributed collection of data grouped into named columns.\n",
    "* pyspark.sql.Column       :               A column expression in a DataFrame.\n",
    "* pyspark.sql.Row          :                A row of data in a DataFrame.\n",
    "* pyspark.sql.GroupedData  :     Aggregation methods, returned by DataFrame.groupBy().\n",
    "* pyspark.sql.DataFrameNaFunctions :         Methods for handling missing data (null values).\n",
    "* pyspark.sql.DataFrameStatFunctions:       Methods for statistics functionality.\n",
    "* pyspark.sql.functions             :   List of built-in functions available for DataFrame.\n",
    "* pyspark.sql.types                 :       List of data types available.\n",
    "* pyspark.sql.Window                :      For working with window functions\n",
    "\n",
    "#### Reference :https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DataFrame Transformations & Actions\n",
    "\n",
    "### Transformations\n",
    "* In Spark, the core data structures are immutable, meaning they cannot be changed after they’re created.\n",
    "* To “change” a DataFrame, you need to instruct Spark how you would like to modify it to do what you want.These instructions are called transformations.This will create another new dataframe of type **_pyspark.sql.dataframe.DataFrame_**\n",
    "* Transformations are the core of how you express your business logic using Spark.\n",
    "* Transformations are simply ways of specifying different series of data manipulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that these return no output. <br>This is because we specified only an abstract transformation, and Spark will not act on transformations until we call an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Actions\n",
    "* Transformations allow us to build up our logical transformation plan. \n",
    "* To trigger the computation, we run an action.\n",
    "    * An action instructs Spark to compute a result from a series of transformations. \n",
    "    * The simplest action is count, which gives us the total number of records in the DataFrame\n",
    "\n",
    "#### There are 3 types of actions\n",
    "Actions to view data in the console.\n",
    "* Actions to collect data to native objects in the respective language\n",
    "* Actions to write to output data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating a  Dataframe from an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Creating the RDD from the local or hdfs file system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID,Product_ID,Gender,Age,Occupation,City_Category,Stay_In_Current_City_Years,Marital_Status,Product_Category_1,Product_Category_2,Product_Category_3\r",
      "\r\n",
      "1000004,P00128942,M,46-50,7,B,2,1,1,11,\r",
      "\r\n",
      "1000009,P00113442,M,26-35,17,C,0,0,3,5,\r",
      "\r\n",
      "1000010,P00288442,F,36-45,1,B,4+,1,5,14,\r",
      "\r\n",
      "1000010,P00145342,F,36-45,1,B,4+,1,4,9,\r",
      "\r\n",
      "1000011,P00053842,F,26-35,1,C,1,0,4,5,12\r",
      "\r\n",
      "1000013,P00350442,M,46-50,1,C,3,1,2,3,15\r",
      "\r\n",
      "1000013,P00155442,M,46-50,1,C,3,1,1,11,15\r",
      "\r\n",
      "1000013,P0094542,M,46-50,1,C,3,1,2,4,9\r",
      "\r\n",
      "1000015,P00161842,M,26-35,7,A,1,0,10,13,16\r",
      "\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/mahidharv/SparkSql_Activity/SalesData/test.csv | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "testRDD = spark.sparkContext.textFile(\"file:///home/mahidharv/SparkSql_Activity/SalesData/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Observe the first few records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'User_ID,Product_ID,Gender,Age,Occupation,City_Category,Stay_In_Current_City_Years,Marital_Status,Product_Category_1,Product_Category_2,Product_Category_3',\n",
       " u'1000004,P00128942,M,46-50,7,B,2,1,1,11,',\n",
       " u'1000009,P00113442,M,26-35,17,C,0,0,3,5,']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The rows are read as list of unicoded strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testRDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Count of the Records including the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Records with header: ', 233600)\n",
      "\n",
      "First Two Records Before Removing Header\n",
      "\n",
      "[u'User_ID,Product_ID,Gender,Age,Occupation,City_Category,Stay_In_Current_City_Years,Marital_Status,Product_Category_1,Product_Category_2,Product_Category_3', u'1000004,P00128942,M,46-50,7,B,2,1,1,11,']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Records with header: \", testRDD.count())\n",
    "print(\"\\nFirst Two Records Before Removing Header\\n\")\n",
    "print(testRDD.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Count of the Records excluding the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Records without header: ', 233599)\n",
      "\n",
      "First Two Records After Removing Header\n",
      "\n",
      "[u'1000004,P00128942,M,46-50,7,B,2,1,1,11,', u'1000009,P00113442,M,26-35,17,C,0,0,3,5,']\n"
     ]
    }
   ],
   "source": [
    "header = testRDD._______()\n",
    "testRDD = testRDD.______(lambda line: line _______)\n",
    "print(\"Total Records without header: \", testRDD.count())\n",
    "print(\"\\nFirst Two Records After Removing Header\\n\")\n",
    "print(testRDD.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the data into individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Two Records After Split/Parsing\n",
      "\n",
      "\n",
      "[[u'1000004', u'P00128942', u'M', u'46-50', u'7', u'B', u'2', u'1', u'1', u'11', u'']]\n"
     ]
    }
   ],
   "source": [
    "splitRDD = testRDD.map(lambda line: line.split( ))\n",
    "print(\"\\nFirst Two Records After Split/Parsing\\n\")\n",
    "print(\"\\n{}\".format(splitRDD.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe for the above Data\n",
    "1. Define Schema\n",
    "2. Create dataframe using the above schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Defining Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will define the schema using __`StructType`__  in the **`pyspark.sql.types`** class\n",
    "* Struct type, consisting of a list of `StructFields`.\n",
    "  * This is the data type representing a `Row`.\n",
    "  * Iterating `StructType` will iterate `StructField`\\s.\n",
    "  * A `StructField` can be accessed by name or position.\n",
    "* Note, the StructField class is broken down in terms of:\n",
    "\t * name : The name of this field\n",
    "\t * dataType : The data type of this field\n",
    "\t * nullable : Indicates whether values of this field can be null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "testSchema = StructType([\n",
    "    StructField(\"User_ID\", StringType(), True),\n",
    "    StructField(\"Product_ID\", StringType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Age\", StringType(), True),\n",
    "    StructField(\"Occupation\", StringType(), True),\n",
    "    StructField(\"City_Category\", StringType(), True),\n",
    "    StructField(\"Stay_In_Current_City_Years\", StringType(), True),\n",
    "    StructField(\"Marital_Status\", StringType(), True),\n",
    "    StructField(\"Product_Category_1\", StringType(), True),\n",
    "    StructField(\"Product_Category_2\", StringType(), True),\n",
    "    StructField(\"Product_Category_3\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(__________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructField(Gender,StringType,true)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSchema[\"Gender\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2. Create DataFrame using the above schema\n",
    "* To create a data frame we need to use the __*createDataFrame*__ method from _**pyspark.sql.SparkSession**_ class.\n",
    "\n",
    "* _**createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)**_\n",
    "* Creates a DataFrame from an RDD, a list or a pandas.DataFrame.\n",
    "   * When schema is a list of column names, the type of each column will be inferred from data.\n",
    "   * When schema is None, it will try to infer the schema (column names and types) from data, which should be an RDD of Row, or namedtuple, or dict.\n",
    "   * If schema inference is needed, samplingRatio is used to determined the ratio of rows used for schema inference. The first row will be used if samplingRatio is None.\n",
    "\n",
    "* _Parameters_ :\t\n",
    "  * data – an RDD of any kind of SQL data representation(e.g. row, tuple, int, boolean, etc.), or list, or pandas.DataFrame.\n",
    "  * schema – a pyspark.sql.types.DataType or a datatype string or a list of column names, default is None. The data type string format equals to pyspark.sql.types.DataType.simpleString, except that top level struct type can omit the struct<> and atomic types use typeName() as their format, e.g. use byte instead of tinyint for pyspark.sql.types.ByteType. We can also use int as a short name for IntegerType.\n",
    "  * samplingRatio – the sample ratio of rows used for inferring\n",
    "  * verifySchema – verify data types of every row against schema.\n",
    "  * Returns:\tDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testDF = spark.createDataFrame(data = splitRDD, schema=testSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[User_ID: string, Product_ID: string, Gender: string, Age: string, Occupation: string, City_Category: string, Stay_In_Current_City_Years: string, Marital_Status: string, Product_Category_1: string, Product_Category_2: string, Product_Category_3: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a csv file :\n",
    "* Reading a CSV file into a DataFrame and converting it to a local array or list of rows.\n",
    "* We will use _**spark.read.csv**_ from _**pyspark.sql.DataFrameReader**_ class\n",
    "* Parameters :\n",
    "    Parameters:\t\n",
    "  * path – string, or list of strings, for input path(s).\n",
    "  * schema – an optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).\n",
    "  * sep – sets a single character as a separator for each field and value. If None is set, it uses the default value, ,.\n",
    "  * encoding – decodes the CSV files by the given encoding type. If None is set, it uses the default value, UTF-8. \n",
    "  * quote – sets a single character used for escaping quoted values where the separator can be part of the value. If None is set, it uses the default value, \". If you would like to turn off quotations, you need to set an empty string.\n",
    "  * escape – sets a single character used for escaping quotes inside an already quoted value. If None is set, it uses the default value, \\.\n",
    "  * comment – sets a single character used for skipping lines beginning with this character. By default (None), it is disabled.\n",
    "  * header – uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "  * inferSchema – infers the input schema automatically from data. It requires one extra pass over the data. If None is set, it uses the default value, false.\n",
    "  * ignoreLeadingWhiteSpace – a flag indicating whether or not leading whitespaces from values being read should be skipped. If None is set, it uses the default value, false.\n",
    "  * ignoreTrailingWhiteSpace – a flag indicating whether or not trailing whitespaces from values being read should be skipped. If None is set, it uses the default value, false.\n",
    "  * nullValue – sets the string representation of a null value. If None is set, it uses the default value, empty string. Since 2.0.1, this nullValue param applies to all supported types including the string type.\n",
    "  * nanValue – sets the string representation of a non-number value. If None is set, it uses the default value, NaN.\n",
    "  * positiveInf – sets the string representation of a positive infinity value. If None is set, it uses the default value, Inf.\n",
    "  * negativeInf – sets the string representation of a negative infinity value. If None is set, it uses the default value, Inf.\n",
    "  * dateFormat – sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.\n",
    "  * timestampFormat – sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd'T'HH:mm:ss.SSSXXX.\n",
    "  * maxColumns – defines a hard limit of how many columns a record can have. If None is set, it uses the default value, 20480.\n",
    "  * maxCharsPerColumn – defines the maximum number of characters allowed for any given value being read. If None is set, it uses the default value, -1 meaning unlimited length.\n",
    "  * maxMalformedLogPerPartition – this parameter is no longer used since Spark 2.2.0. If specified, it is ignored.\n",
    "  * mode –\n",
    "    * allows a mode for dealing with corrupt records during parsing. If None isset, it uses the default value, PERMISSIVE.\n",
    "    * PERMISSIVE : sets other fields to null when it meets a corrupted record, and puts the malformed string into a field configured by columnNameOfCorruptRecord. To keep corrupt records, an user can set a string type field named columnNameOfCorruptRecord in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When a length of parsed CSV tokens is shorter than an expected length of a schema, it sets null for extra fields.\n",
    "    * DROPMALFORMED : ignores the whole corrupted records.\n",
    "    * FAILFAST : throws an exception when it meets corrupted records.\n",
    "    * columnNameOfCorruptRecord – allows renaming the new field having malformed string created by PERMISSIVE mode. This overrides spark.sql.columnNameOfCorruptRecord. If None is set, it uses the value specified in spark.sql.columnNameOfCorruptRecord.\n",
    "  * multiLine – parse one record, which may span multiple lines. If None is set, it uses the default value, false.\n",
    "  * charToEscapeQuoteEscaping – sets a single character used for escaping the escape for the quote character. If None is set, the default value is escape character when escape and quote characters are different, \\ otherwise..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainDF = spark.read.csv(header=True,\n",
    "                         inferSchema=True,\n",
    "                         path=\"file:///home/mahidharv/SparkSql_Activity/SalesData/train.csv\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  We can also use the generalized approach of using _**spark.read.format**_ and use the options to load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(\"file:///home/mahidharv/SparkSql_Activity/SalesData/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print Schema : prints out the schema in tree format\n",
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To Show first n observations\n",
    "*  Use head operation to see first n observations (say, 2 observations). \n",
    "*  Head operation in PySpark is similar to head operation in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6, Product_Category_3=14, Purchase=15200)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Above results are comprised of row like format. \n",
    "*  To see the result in more interactive manner (rows under the columns), Use the show operation. \n",
    "* Show operation on train and take first 5 rows of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|Age |Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001|P00069042 |F     |0-17|10        |A            |2                         |0             |3                 |null              |null              |8370    |\n",
      "|1000001|P00248942 |F     |0-17|10        |A            |2                         |0             |1                 |6                 |14                |15200   |\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To Count the number of rows in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records count in train dataset is 550068\n",
      "Total records count in test dataset is 233599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Total records count in train dataset is {}'.format(trainDF.count()))\n",
    "print('Total records count in test dataset is {}'.format(testDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Columns count and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns count in train dataset is 12\n",
      "\n",
      "\n",
      "Columns in train dataset are: ['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Purchase'] \n",
      "\n",
      "Total Columns count in test dataset is 11\n",
      "\n",
      "\n",
      "Columns in test dataset are: ['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total Columns count in train dataset is {}\".format(len(trainDF.columns)))\n",
    "print(\"\\n\\nColumns in train dataset are: {} \\n\".format(trainDF.columns))\n",
    "\n",
    "print(\"Total Columns count in test dataset is {}\".format(len(testDF.columns)))\n",
    "print(\"\\n\\nColumns in test dataset are: {} \\n\".format(testDF.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           User_ID|Product_ID|Gender|   Age|       Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            550068|    550068|550068|550068|           550068|       550068|                    550068|             550068|            550068|            376430|            166821|           550068|\n",
      "|   mean|1003028.8424013031|      null|  null|  null|8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 9.842329251122386|12.668243206790512|9263.968712959126|\n",
      "| stddev|1727.5915855312976|      null|  null|  null|6.522660487341822|         null|        0.9890866807573172|0.49177012631733175| 3.936211369201386| 5.086589648693479| 4.125337631575277|5023.065393820575|\n",
      "|    min|           1000001| P00000142|     F|  0-17|                0|            A|                         0|                  0|                 1|                 2|                 3|               12|\n",
      "|    max|           1006040|  P0099942|     M|   55+|               20|            C|                        4+|                  1|                20|                18|                18|            23961|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To get the summary statistics (mean, standard deviance, min ,max , count) of numerical columns in a DataFrame\n",
    "trainDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|summary|Product_ID|\n",
      "+-------+----------+\n",
      "|  count|    550068|\n",
      "|   mean|      null|\n",
      "| stddev|      null|\n",
      "|    min| P00000142|\n",
      "|    max|  P0099942|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check what happens when we specify the name of a categorical / String columns in describe operation.\n",
    "## describe operation is working for String type column but the output for mean, stddev are null and \n",
    "## min & max values are calculated based on ASCII value of categories.\n",
    "trainDF.describe('Product_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "* With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure SQL. \n",
    "* There is no performance difference between writing SQL queries or writing DataFrame code, they both “compile” to the same underlying plan that we specify in DataFrame code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database global_temp, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark SQL Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"\"\"create database mahidhar\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"use mahidhar\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"\"\"CREATE TABLE user_test (i INT) \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"use mahidhar\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|mahidhar|    sales|      false|\n",
      "|mahidhar|       t3|      false|\n",
      "|mahidhar|       t4|      false|\n",
      "|mahidhar|user_test|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"show tables\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  i|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from _____\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create view/table\n",
    "trainDF.createOrReplaceTempView(\"trainDFTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verify Dataframe\n",
    "trainDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6, Product_Category_3=14, Purchase=15200)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verify Dataframe\n",
    "trainDF.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verify Table\n",
    "spark.sql(\"SELECT * FROM trainDFTable LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Comparison Spark DataFrame vs Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Age#123], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Age#123, 200)\n",
      "   +- *(1) HashAggregate(keys=[Age#123], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [Age#123] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/mahidharv/SparkSql_Activity/SalesData/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Age:string>\n"
     ]
    }
   ],
   "source": [
    "#dataframeWay = trainDF.where(trainDF.Purchase>15000).count()\n",
    "dataframeWay = trainDF.groupBy('Age').count()\n",
    "dataframeWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Age#123], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Age#123, 200)\n",
      "   +- *(1) HashAggregate(keys=[Age#123], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [Age#123] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/mahidharv/SparkSql_Activity/SalesData/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Age:string>\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"SELECT Age, count(1) FROM trainDFTable GROUP BY Age\")\n",
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select & SelectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "| userID|User_ID|User_ID|User_ID|\n",
      "+-------+-------+-------+-------+\n",
      "|1000001|1000001|1000001|1000001|\n",
      "|1000001|1000001|1000001|1000001|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Multiple ways of referring a column in a dataframe\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "trainDF.select(expr(\"User_ID AS userID\") , col(\"User_ID\"), column(\"User_ID\"), \"User_ID\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| userID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(expr(\"User_ID AS userID\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| userID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT User_ID AS userID FROM trainDFTable\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "| userID|productID|\n",
      "+-------+---------+\n",
      "|1000001|P00069042|\n",
      "|1000001|P00248942|\n",
      "+-------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.selectExpr(\"User_ID AS userID\", \"Product_ID AS productID\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+\n",
      "|User_ID|Product_ID| Age|\n",
      "+-------+----------+----+\n",
      "|1000001| P00069042|0-17|\n",
      "|1000001| P00248942|0-17|\n",
      "+-------+----------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(\"User_ID\", \"Product_ID\", \"Age\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to Spark Types (Literals)\n",
    "Sometimes we need to pass explicit values into Spark that aren’t a new column but are just a value in all the rows. This might be a constant value or something we’ll need to compare to later on. The way we do this is through literals. \n",
    "This is basically a translation from a given programming language’s literal value to one that Spark understands. \n",
    "Literals are expressions and can be used in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "trainDF.select(\"*\", lit(1).alias('One')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## In SQL, literals are just the specific value.\n",
    "spark.sql(\"SELECT *, 1 as One FROM trainDFTable LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## More Formal way\n",
    "trainDF.withColumn(\"One\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|SameCategoryCode|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|            null|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|           false|\n",
      "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|            null|\n",
      "|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|                14|              null|    1057|           false|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF = trainDF.withColumn(\"SameCategoryCode\", trainDF[\"Product_Category_1\"] == trainDF[\"Product_Category_2\"])\n",
    "tempDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---------------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|SimilarCategory|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---------------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|           null|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|          false|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.withColumnRenamed(\"SameCategoryCode\", \"SimilarCategory\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.drop(\"SameCategoryCode\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing a Column’s Type (cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      " |-- SameCategoryCode: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: string (nullable = true)\n",
      " |-- SameCategoryCode: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.withColumn(\"Purchase\", col(\"Purchase\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in Product_ID's in train dataset are 3631\n",
      "Distinct values in Product_ID's in test dataset are 3491\n"
     ]
    }
   ],
   "source": [
    "## To find the number of distinct product in train and test datasets\n",
    "## To calculate the number of distinct products in train and test datasets apply distinct operation.\n",
    "print(\"Distinct values in Product_ID's in train dataset are {}\".format(trainDF.select('Product_ID').distinct().count()))\n",
    "print(\"Distinct values in Product_ID's in test dataset are {}\".format(testDF.select('Product_ID').distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences in two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Product_ID's there in test dataset but not train dataset are 46\n",
      "Count of Product_ID's there in train dataset but not test dataset are 186\n"
     ]
    }
   ],
   "source": [
    "## From the above we can see the train file has more categories than test file. \n",
    "## Let us check what are the categories for Product_ID, which are in test file but not in train file by \n",
    "## applying subtract operation.\n",
    "## We can do the same for all categorical features.\n",
    "diff_cat_in_test_train=testDF.select('Product_ID').subtract(trainDF.select('Product_ID'))\n",
    "print(\"Count of Product_ID's there in test dataset but not train dataset are {}\".format(diff_cat_in_test_train.count()))\n",
    "\n",
    "diff_cat_in_train_test=trainDF.select('Product_ID').subtract(testDF.select('Product_ID'))\n",
    "print(\"Count of Product_ID's there in train dataset but not test dataset are {}\".format(diff_cat_in_train_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair wise Frequencies - Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|Age_Gender|    F|     M|\n",
      "+----------+-----+------+\n",
      "|      0-17| 5083| 10019|\n",
      "|     46-50|13199| 32502|\n",
      "|     18-25|24628| 75032|\n",
      "|     36-45|27170| 82843|\n",
      "|       55+| 5083| 16421|\n",
      "|     51-55| 9894| 28607|\n",
      "|     26-35|50752|168835|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To calculate pair wise frequency of categorical columns\n",
    "## Use crosstab operation on DataFrame to calculate the pair wise frequency of columns. \n",
    "## Apply crosstab operation on ‘Age’ and ‘Gender’ columns of train DataFrame.\n",
    "trainDF.crosstab('Age', 'Gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+-----+\n",
      "|Age_Marital_Status|     0|    1|\n",
      "+------------------+------+-----+\n",
      "|              0-17| 15102|    0|\n",
      "|             46-50| 12690|33011|\n",
      "|             18-25| 78544|21116|\n",
      "|             36-45| 66377|43636|\n",
      "|               55+|  7883|13621|\n",
      "|             51-55| 10839|27662|\n",
      "|             26-35|133296|86291|\n",
      "+------------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.crosstab('Age','Marital_Status').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|  Age|Gender| count|\n",
      "+-----+------+------+\n",
      "|51-55|     F|  9894|\n",
      "|18-25|     M| 75032|\n",
      "| 0-17|     F|  5083|\n",
      "|46-50|     M| 32502|\n",
      "|18-25|     F| 24628|\n",
      "|  55+|     M| 16421|\n",
      "|  55+|     F|  5083|\n",
      "|36-45|     M| 82843|\n",
      "|26-35|     F| 50752|\n",
      "| 0-17|     M| 10019|\n",
      "|36-45|     F| 27170|\n",
      "|51-55|     M| 28607|\n",
      "|26-35|     M|168835|\n",
      "|46-50|     F| 13199|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy('Age', 'Gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|  Age|    F|     M|\n",
      "+-----+-----+------+\n",
      "|18-25|24628| 75032|\n",
      "|26-35|50752|168835|\n",
      "| 0-17| 5083| 10019|\n",
      "|46-50|13199| 32502|\n",
      "|51-55| 9894| 28607|\n",
      "|36-45|27170| 82843|\n",
      "|  55+| 5083| 16421|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select Age,\n",
    "    sum(case when Gender = 'F' then 1 else 0 end) F,\n",
    "    sum(case when Gender = 'M' then 1 else 0 end) M\n",
    "from trainDFTable\n",
    "group by Age\"\"\").show()\n",
    "\n",
    "# spark.sql(\"\"\"select Age,\n",
    "#     count(*) total,\n",
    "#     sum(case when Gender = 'F' then 1 else 0 end) F,\n",
    "#     sum(case when Gender = 'M' then 1 else 0 end) M\n",
    "# from trainDFTable\n",
    "# group by Age\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|  Age|Gender|\n",
      "+-----+------+\n",
      "|51-55|     F|\n",
      "|18-25|     M|\n",
      "| 0-17|     F|\n",
      "|46-50|     M|\n",
      "|18-25|     F|\n",
      "|  55+|     M|\n",
      "|  55+|     F|\n",
      "|36-45|     M|\n",
      "|26-35|     F|\n",
      "| 0-17|     M|\n",
      "|36-45|     F|\n",
      "|51-55|     M|\n",
      "|26-35|     M|\n",
      "|46-50|     F|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##To get the DataFrame without any duplicate rows of given a DataFrame\n",
    "##Use dropDuplicates operation to drop the duplicate rows of a DataFrame. \n",
    "## In this command, performing this on two columns Age and Gender of train dataset and \n",
    "## Get the all unique rows for these two columns.\n",
    "trainDF.select('Age','Gender').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Nulls in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "* **To drop the all rows with null value?**\n",
    "* Use dropna operation. \n",
    "* To drop row from the DataFrame it consider three options.\n",
    "    * how – ‘any’ or ‘all’. \n",
    "        * If ‘any’, drop a row if it contains any nulls. \n",
    "        * If ‘all’, drop a row only if all its values are null.\n",
    "    * thresh – int, default None If specified, drop rows that have less than thresh non-null values.This overwrites the how parameter.\n",
    "    * subset – optional list of column names to consider.\n",
    "\n",
    "* Drop null rows in train with default parameters and count the rows in output DataFrame. \n",
    "* Default options are any, None, None for how, thresh, subset respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166821\n",
      "166821\n",
      "166821\n"
     ]
    }
   ],
   "source": [
    "print(trainDF.dropna().count())\n",
    "print(trainDF.na.drop().count())\n",
    "print(trainDF.na.drop(\"any\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|Age |Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001|P00069042 |F     |0-17|10        |A            |2                         |0             |3                 |-1                |-1                |8370    |\n",
      "|1000001|P00248942 |F     |0-17|10        |A            |2                         |0             |1                 |6                 |14                |15200   |\n",
      "|1000001|P00087842 |F     |0-17|10        |A            |2                         |0             |12                |-1                |-1                |1422    |\n",
      "|1000001|P00085442 |F     |0-17|10        |A            |2                         |0             |12                |14                |-1                |1057    |\n",
      "|1000002|P00285442 |M     |55+ |16        |C            |4+                        |0             |8                 |-1                |-1                |7969    |\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To replace the null values in DataFrame with constant number\n",
    "## Use fillna operation. \n",
    "\n",
    "##The fillna will take two parameters to fill the null values.\n",
    "## value:\n",
    "##     It will take a dictionary to specify which column will replace with which value.\n",
    "##     A value (int , float, string) for all columns.\n",
    "##subset: Specify some selected columns.\n",
    "\n",
    "##Fill ‘-1’ inplace of null values in train DataFrame.\n",
    "trainDF.fillna(-1).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Filling with different values for different columns\n",
    "fill_cols_vals = {\n",
    "\"Gender\": 'M',\n",
    "\"Purchase\" : 999999\n",
    "}\n",
    "trainDF.na.fill(fill_cols_vals).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.na.replace([\"\"], [\"UNKNOWN\"], \"Gender\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n"
     ]
    }
   ],
   "source": [
    "## To filter the rows in train dataset which has Purchases more than 15000\n",
    "## apply the filter operation on Purchase column in train DataFrame \n",
    "## to filter out the rows with values more than 15000. \n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(trainDF.Purchase > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(col(\"Purchase\") > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(column(\"Purchase\") > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(expr(\"Purchase\") > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\".format(trainDF.filter(trainDF[\"Purchase\"] > 15000).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| Count|\n",
      "+------+\n",
      "|110523|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "COUNT(*) AS Count\n",
    "FROM trainDFTable\n",
    "WHERE Purchase > 15000\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.where(\"Purchase > 15000\").where(\"Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.filter(\"Purchase > 15000\").where(\"Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.where((col(\"Purchase\") > 15000) & (col(\"Gender\") == 'F')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.filter((col(\"Purchase\") > 15000) & (col(\"Gender\") == 'F')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM trainDFTable WHERE Purchase > 15000 AND Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT Age)|\n",
      "+-------------------+\n",
      "|                  7|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "trainDF.select(countDistinct(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|approx_count_distinct(Age)|\n",
      "+--------------------------+\n",
      "|                         7|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "trainDF.select(approx_count_distinct(\"Age\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First and Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------+\n",
      "|first(Product_ID, false)|last(Product_ID, false)|\n",
      "+------------------------+-----------------------+\n",
      "|               P00069042|              P00371644|\n",
      "+------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "trainDF.select(first(\"Product_ID\",), last(\"Product_ID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min and Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Purchase)|max(Purchase)|\n",
      "+-------------+-------------+\n",
      "|           12|        23961|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "trainDF.select(min(\"Purchase\"), max(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Purchase)|\n",
      "+-------------+\n",
      "|   5095812742|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "trainDF.select(sum(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Purchase)|\n",
      "+----------------------+\n",
      "|             208520914|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "trainDF.select(sumDistinct(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------+-----------------+\n",
      "|(total_purchases / total_transactions)|    avg_purchases|   mean_purchases|\n",
      "+--------------------------------------+-----------------+-----------------+\n",
      "|                     9263.968712959126|9263.968712959126|9263.968712959126|\n",
      "+--------------------------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "trainDF.select(\n",
    "    count(\"Purchase\").alias(\"total_transactions\"),\n",
    "    sum(\"Purchase\").alias(\"total_purchases\"),\n",
    "    avg(\"Purchase\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Purchase)\").alias(\"mean_purchases\"))\\\n",
    "  .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+---------------------+\n",
      "|  var_pop(Purchase)|  var_samp(Purchase)|stddev_pop(Purchase)|stddev_samp(Purchase)|\n",
      "+-------------------+--------------------+--------------------+---------------------+\n",
      "|2.523114008138541E7|2.5231185950597852E7|   5023.060827959921|    5023.065393820575|\n",
      "+-------------------+--------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "trainDF.select(var_pop(\"Purchase\"), var_samp(\"Purchase\"),\n",
    "  stddev_pop(\"Purchase\"), stddev_samp(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------------------------------+------------------------------------+-------------------------------------+\n",
      "|var_pop(CAST(Purchase AS DOUBLE))|var_samp(CAST(Purchase AS DOUBLE))|stddev_pop(CAST(Purchase AS DOUBLE))|stddev_samp(CAST(Purchase AS DOUBLE))|\n",
      "+---------------------------------+----------------------------------+------------------------------------+-------------------------------------+\n",
      "|              2.523114008138541E7|              2.5231185950597852E7|                   5023.060827959921|                    5023.065393820575|\n",
      "+---------------------------------+----------------------------------+------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT var_pop(Purchase), var_samp(Purchase),\n",
    "             stddev_pop(Purchase), stddev_samp(Purchase)\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skewness and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|skewness(Purchase)|  kurtosis(Purchase)|\n",
      "+------------------+--------------------+\n",
      "|0.6001383671643461|-0.33838539753607577|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "trainDF.select(skewness(\"Purchase\"), kurtosis(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|skewness(CAST(Purchase AS DOUBLE))|kurtosis(CAST(Purchase AS DOUBLE))|\n",
      "+----------------------------------+----------------------------------+\n",
      "|                0.6001383671643461|              -0.33838539753607577|\n",
      "+----------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT skewness(Purchase), kurtosis(Purchase)\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------------+---------------------------------------+\n",
      "|corr(Product_Category_1, Purchase)|covar_samp(Product_Category_1, Purchase)|covar_pop(Product_Category_1, Purchase)|\n",
      "+----------------------------------+----------------------------------------+---------------------------------------+\n",
      "|              -0.34370334591990875|                     -6795.6500072045765|                     -6795.637653004719|\n",
      "+----------------------------------+----------------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "trainDF.select(corr(\"Product_Category_1\", \"Purchase\"), covar_samp(\"Product_Category_1\", \"Purchase\"),\n",
    "    covar_pop(\"Product_Category_1\", \"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------+\n",
      "|corr(CAST(Product_Category_1 AS DOUBLE), CAST(Purchase AS DOUBLE))|covar_samp(CAST(Product_Category_1 AS DOUBLE), CAST(Purchase AS DOUBLE))|covar_pop(CAST(Product_Category_1 AS DOUBLE), CAST(Purchase AS DOUBLE))|\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------+\n",
      "|                                              -0.34370334591990875|                                                     -6795.6500072045765|                                                     -6795.637653004719|\n",
      "+------------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT corr(Product_Category_1, Purchase), covar_samp(Product_Category_1, Purchase),\n",
    "             covar_pop(Product_Category_1, Purchase)\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|    collect_set(Age)|   collect_list(Age)|\n",
      "+--------------------+--------------------+\n",
      "|[55+, 51-55, 0-17...|[0-17, 0-17, 0-17...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "trainDF.agg(collect_set(\"Age\"), collect_list(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|    collect_set(Age)|   collect_list(Age)|\n",
      "+--------------------+--------------------+\n",
      "|[55+, 51-55, 0-17...|[0-17, 0-17, 0-17...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT collect_set(Age), collect_list(Age) FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|  Age|Gender| count|\n",
      "+-----+------+------+\n",
      "|51-55|     F|  9894|\n",
      "|18-25|     M| 75032|\n",
      "| 0-17|     F|  5083|\n",
      "|46-50|     M| 32502|\n",
      "|18-25|     F| 24628|\n",
      "|  55+|     M| 16421|\n",
      "|  55+|     F|  5083|\n",
      "|36-45|     M| 82843|\n",
      "|26-35|     F| 50752|\n",
      "| 0-17|     M| 10019|\n",
      "|36-45|     F| 27170|\n",
      "|51-55|     M| 28607|\n",
      "|26-35|     M|168835|\n",
      "|46-50|     F| 13199|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy(\"Age\", \"Gender\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping with Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------------+\n",
      "|  Age|  quan|count(Purchase)|\n",
      "+-----+------+---------------+\n",
      "|18-25| 99660|          99660|\n",
      "|26-35|219587|         219587|\n",
      "| 0-17| 15102|          15102|\n",
      "|46-50| 45701|          45701|\n",
      "|51-55| 38501|          38501|\n",
      "|36-45|110013|         110013|\n",
      "|  55+| 21504|          21504|\n",
      "+-----+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy(\"Age\").agg(\n",
    "  count(\"Purchase\").alias(\"quan\"),\n",
    "  expr(\"count(Purchase)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping with Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+--------------------+\n",
      "|  Age|    avg(Purchase)|stddev_pop(Purchase)|\n",
      "+-----+-----------------+--------------------+\n",
      "|18-25|9169.663606261289|   5034.296739627788|\n",
      "|26-35|9252.690632869888|   5010.515894010142|\n",
      "| 0-17|8933.464640444974|   5110.944823427657|\n",
      "|46-50|9208.625697468327|   4967.162022122699|\n",
      "|51-55|9534.808030960236|   5087.302011173861|\n",
      "|36-45|9331.350694917874|   5022.901050378551|\n",
      "|  55+|9336.280459449405|   5011.377469555766|\n",
      "+-----+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy(\"Age\").agg(expr(\"avg(Purchase)\"),expr(\"stddev_pop(Purchase)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|  Age|    avg(Purchase)|\n",
      "+-----+-----------------+\n",
      "|18-25|9169.663606261289|\n",
      "|26-35|9252.690632869888|\n",
      "| 0-17|8933.464640444974|\n",
      "|46-50|9208.625697468327|\n",
      "|51-55|9534.808030960236|\n",
      "|36-45|9331.350694917874|\n",
      "|  55+|9336.280459449405|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To find the mean of each age group in train dataset - Average purchases in each age group\n",
    "trainDF.groupby('Age').agg({'Purchase': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|  Age|sum(Purchase)|\n",
      "+-----+-------------+\n",
      "|18-25|    913848675|\n",
      "|26-35|   2031770578|\n",
      "| 0-17|    134913183|\n",
      "|46-50|    420843403|\n",
      "|51-55|    367099644|\n",
      "|36-45|   1026569884|\n",
      "|  55+|    200767375|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupby('Age').agg({'Purchase': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------------+-------------------+-------------+------------+---------------+-------------------------------+-----------------------+--------+-----------+-----------------------+---------------+\n",
      "|  Age|sum(City_Category)|sum(Product_Category_3)|sum(Marital_Status)|sum(Purchase)|sum(User_ID)|sum(Occupation)|sum(Stay_In_Current_City_Years)|sum(Product_Category_1)|sum(Age)|sum(Gender)|sum(Product_Category_2)|sum(Product_ID)|\n",
      "+-----+------------------+-----------------------+-------------------+-------------+------------+---------------+-------------------------------+-----------------------+--------+-----------+-----------------------+---------------+\n",
      "|18-25|              null|                 388041|              21116|    913848675| 99939196632|         671348|                       116997.0|                 509371|    null|       null|                 654936|           null|\n",
      "|26-35|              null|                 846624|              86291|   2031770578|220270500414|        1734073|                       275611.0|                1166945|    null|       null|                1473278|           null|\n",
      "| 0-17|              null|                  57725|                  0|    134913183| 15143112813|         132309|                        20320.0|                  76775|    null|       null|                  96155|           null|\n",
      "|46-50|              null|                 173059|              33011|    420843403| 45846804203|         389239|                        51742.0|                 262424|    null|       null|                 315572|           null|\n",
      "|51-55|              null|                 146334|              27662|    367099644| 38615925320|         339198|                        44243.0|                 222313|    null|       null|                 267570|           null|\n",
      "|36-45|              null|                 424412|              43636|   1026569884|110350311441|         972225|                       148162.0|                 604438|    null|       null|                 750081|           null|\n",
      "|  55+|              null|                  77134|              13621|    200767375| 21568218459|         204346|                        26277.0|                 130450|    null|       null|                 147356|           null|\n",
      "+-----+------------------+-----------------------+-------------------+-------------+------------+---------------+-------------------------------+-----------------------+--------+-----------+-----------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Apply sum, min, max, count with groupby to get different summary insight for each group. \n",
    "exprs = {x: \"sum\" for x in trainDF.columns}\n",
    "trainDF.groupBy(\"Age\").agg(exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "### Data dictionary :\n",
    "* __RestGenInfo.csv__ contains :\n",
    "    * placeID - Uniqued Id of restaurants\n",
    "    * latitude - Location detail \n",
    "    * longitude - Location detail\n",
    "    * name - Name of the restaurant\n",
    "    * state - Name of the state \n",
    "    * alcohol - Constraints on having alcoholic beverages\n",
    "    * smoking_area - Information for smokers\n",
    "    * price - Pricing type of restaurant\n",
    "    * franchise - Does the restaurant have frachise\n",
    "    * area - open or close type of restaurant\n",
    "\n",
    "* __Cuisine.csv__ contains :\n",
    "    * placeID - Uniqued Id of restaurants\n",
    "    * Rcuisine - Different styles of food\n",
    "\n",
    "    \n",
    "* __PaymentMode.csv__ contains :\n",
    "    * placeID - Uniqued Id of restaurants\n",
    "    * Rpayment - Different modes of payment\n",
    "\n",
    "    \n",
    "* __parking.csv__ contains :\n",
    "     * placeID - Uniqued Id of restaurants\n",
    "     * parking_lot - Different types of parking available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dsets_path = \"academics/Batch44/CSE7322c/20181007_Batch 44_CSE 7322c_SparkSQL and DataFrames_Lab/\"\n",
    "/home/mahidharv/academics/Batch44/CSE7322c/20181007_Batch 44_CSE 7322c_SparkSQL and DataFrames_Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Path does not exist: hdfs://bigdata/user/mahidharv/academics/Batch44/CSE7322c/20181007_Batch 44_CSE 7322c_SparkSQL and DataFrames_Lab/Datasets/RestGenInfo.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-14e7d4b9964e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrestoGen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsets_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Datasets/RestGenInfo.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcuisine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsets_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Datasets/Cuisine.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpaymentMode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsets_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Datasets/PaymentMode.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsets_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Datasets/parking.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Path does not exist: hdfs://bigdata/user/mahidharv/academics/Batch44/CSE7322c/20181007_Batch 44_CSE 7322c_SparkSQL and DataFrames_Lab/Datasets/RestGenInfo.csv;'"
     ]
    }
   ],
   "source": [
    "restoGen = spark.read.csv(dsets_path+'Datasets/RestGenInfo.csv', header=True, inferSchema=True,nullValue='?')\n",
    "cuisine = spark.read.csv(dsets_path+'Datasets/Cuisine.csv', header=True, inferSchema=True)\n",
    "paymentMode = spark.read.csv(dsets_path+'Datasets/PaymentMode.csv', header=True, inferSchema=True)\n",
    "parking = spark.read.csv(dsets_path+'Datasets/parking.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoGen.select([count(when(isnan(c)| col(c).isNull(), 1)).alias(c) for c in restoGen.columns]).show()\n",
    "cuisine.select([count(when(isnan(c)| col(c).isNull(), 1)).alias(c) for c in cuisine.columns]).show()\n",
    "paymentMode.select([count(when(isnan(c)| col(c).isNull(), 1)).alias(c) for c in paymentMode.columns]).show()\n",
    "parking.select([count(when(isnan(c)| col(c).isNull(), 1)).alias(c) for c in parking.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoGen = restoGen.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoGen.select('placeID').distinct().count()\n",
    "\n",
    "cuisine.select('placeID').distinct().count()\n",
    "\n",
    "cuisine.select('Rcuisine').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoGen.createOrReplaceTempView('restoGenTable')\n",
    "cuisine.createOrReplaceTempView('cuisineTable')\n",
    "paymentMode.createOrReplaceTempView('paymentModeTable')\n",
    "parking.createOrReplaceTempView('parkingTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##The  count of restaurants(as numberOfHotels) for each payment modes and area and order based on numberOfHotels in descending order.\n",
    "    \n",
    "spark.sql('''select  count(*) as numberOfHotels, Rpayment, area from\n",
    "restoGenTable a join paymentModeTable b \n",
    "where a.placeID = b.placeID group by Rpayment, area \n",
    "order by numberOfHotels desc''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join = restoGen.join(paymentMode, restoGen.placeID == paymentMode.placeID,how='inner') \n",
    "inner_join.show(4)\n",
    "\n",
    "count_of_hotels = inner_join.select('Rpayment','area').groupby('area','Rpayment').count()\n",
    "\n",
    "count_of_hotels = count_of_hotels.withColumnRenamed('count','NumberofHotels')\n",
    "count_of_hotels.show()\n",
    "\n",
    "count_of_hotels.orderBy(count_of_hotels.NumberofHotels.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Count the number of Cuisines that are used by the Restaurants\n",
    "print(\"The number of Disintct Cuisines =  \",cuisine.select('Rcuisine').distinct().count())\n",
    "left_join = restoGen.join(cuisine,how = 'left',on=restoGen.placeID==cuisine.placeID)\n",
    "left_join.select(countDistinct('Rcuisine')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the distinct restaurant names which has valet parking\n",
    "spark.sql('''select distinct name ,parking_lot from restoGenTable a join parkingTable b where a.placeID = b.placeID and \n",
    "          b.parking_lot = 'valet parking' ''').show()\n",
    "\n",
    "right_join = restoGen.join(other=parking,on=parking.placeID==restoGen.placeID,how='right')\n",
    "names_of_restaurants = right_join.select('name','parking_lot').filter(parking.parking_lot=='valet parking')\n",
    "names_of_restaurants.distinct().filter(names_of_restaurants.name!='null').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Joins\n",
    "Natural joins make implicit guesses at the columns on which you would like to join. \n",
    "It finds matching columns and returns the results. \n",
    "Left, right, and outer natural joins are all supported.\n",
    "\n",
    "WARNING:\n",
    "Implicit is always dangerous! \n",
    "The following query will give us incorrect results because \n",
    "the two DataFrames/tables share a column name (id), but it means different things in the datasets. \n",
    "You should always use this join with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.sql('''select  * from restoGenTable a NATURAL JOIN paymentModeTable b ''').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross (Cartesian) Joins\n",
    "Cross-joins in simplest terms are inner joins that do not specify a predicate. \n",
    "Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame. \n",
    "This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. \n",
    "If you have 1,000 rows in each DataFrame, the cross-join of these will result in 1,000,000 (1,000 x 1,000) rows. \n",
    "For this reason, you must very explicitly state that you want a cross-join by using the cross join keyword:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tableA.crossJoin(tableB).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## To create a sample DataFrame from the base DataFrame\n",
    "## Use sample operation to take sample of a DataFrame. \n",
    "## The sample method on DataFrame will return a DataFrame containing the sample of base DataFrame. \n",
    "## The sample method takes 3 parameters.\n",
    "## withReplacement = True or False to select a observation with or without replacement.\n",
    "## fraction = x, where x = .5 shows that we want to have 50% data in sample DataFrame.\n",
    "## seed to reproduce the result\n",
    "sampleDF1 = trainDF.sample(False, 0.2, 1234)\n",
    "sampleDF2 = trainDF.sample(False, 0.2, 4321)\n",
    "print(sampleDF1.count(), sampleDF2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitDF = trainDF.randomSplit([0.7, 0.3], seed=1234)\n",
    "print(splitDF[0].count())\n",
    "print(splitDF[1].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## To apply map operation on DataFrame columns\n",
    "## Apply a function on each row of DataFrame using map operation. \n",
    "## After applying this function, we get the result in the form of RDD. \n",
    "## Apply a map operation on User_ID column of train and print the first 5 elements of mapped RDD(x,1) \n",
    "## ----- Applying lambda function.\n",
    "\n",
    "trainDF.select('User_ID').rdd.map(lambda x:(x,1)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Prior to Spark 2.0, spark_df.map would alias to spark_df.rdd.map(). \n",
    "With Spark 2.0, you must explicitly call .rdd first.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## To sort the DataFrame based on column(s)\n",
    "## Use orderBy operation on DataFrame to get sorted output based on some column. \n",
    "## The orderBy operation take two arguments.\n",
    "## List of columns.\n",
    "## ascending = True or False for getting the results in ascending or descending order(list in case of more than two columns )\n",
    "## Sort the train DataFrame based on ‘Purchase’.\n",
    "trainDF.orderBy(trainDF.Purchase.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repartition and Coalesce\n",
    "Another important optimization opportunity is to partition the data according to some frequently filtered columns\n",
    "which controls the physical layout of data across the cluster including the partitioning scheme and the number of\n",
    "partitions.\n",
    "\n",
    "Repartition will incur a full shuffle of the data, regardless of whether or not one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of\n",
    "partitions or when you are looking to partition by a set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find existing partitions count\n",
    "trainDF.rdd.getNumPartitions()\n",
    "## Do the repartition\n",
    "## trainDF.repartition(5)\n",
    "\n",
    "## Repartition based on a column\n",
    "## If we know we are going to be filtering by a certain column often, \n",
    "## it can be worth repartitioning based on that column.\n",
    "## trainDF.repartition(col(“Purchase”))\n",
    "\n",
    "## We can optionally specify the number of partitions we would like too.\n",
    "## trainDF.repartition(5, col(“Purchase”))\n",
    "\n",
    "## Coalesce on the other hand will not incur a full shuffle and will try to combine partitions. \n",
    "## This operation will shuffle our data into 5 partitions based on the Purchase, \n",
    "## then coalesce them (without a full shuffle).\n",
    "## trainDF.repartition(5, col(\"Purchase\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "DataFrame-1\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|   Alex| 25|\n",
      "|  3|  Carol| 53|\n",
      "|  5|  Emily| 25|\n",
      "|  7|Gabriel| 32|\n",
      "|  9|   Ilma| 35|\n",
      "| 11|    Kim| 45|\n",
      "+---+-------+---+\n",
      "\n",
      "None\n",
      "DataFrame-2\n",
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  2|   Ben| 66|\n",
      "|  4|Daniel| 28|\n",
      "|  6| Frank| 64|\n",
      "|  8|Harley| 29|\n",
      "| 10|  Jack| 35|\n",
      "| 12|Litmya| 45|\n",
      "+---+------+---+\n",
      "\n",
      "None\n",
      "After\n",
      "DataFrame-1\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|   Alex| 25|\n",
      "|  3|  Carol| 53|\n",
      "|  5|  Emily| 25|\n",
      "|  7|Gabriel| 32|\n",
      "|  9|   Ilma| 35|\n",
      "| 11|    Kim| 45|\n",
      "|  2|    Ben| 66|\n",
      "|  4| Daniel| 28|\n",
      "|  6|  Frank| 64|\n",
      "|  8| Harley| 29|\n",
      "| 10|   Jack| 35|\n",
      "| 12| Litmya| 45|\n",
      "+---+-------+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[1, 'Alex', 25],[3, 'Carol', 53],[5, 'Emily', 25],[7, 'Gabriel', 32],[9, 'Ilma', 35],[11, 'Kim', 45]], ['id', 'name', 'age'])\n",
    "df2 = spark.createDataFrame([[2, 'Ben', 66],[4, 'Daniel', 28],[6, 'Frank', 64],[8, 'Harley', 29],[10, 'Jack', 35],[12, 'Litmya', 45]], ['id', 'name', 'age'])\n",
    "print(\"Before\")\n",
    "print(\"DataFrame-1\")\n",
    "print(df1.show())\n",
    "print(\"DataFrame-2\")\n",
    "print(df2.show())\n",
    "print(\"After\")\n",
    "df1 = df1.union(df2)\n",
    "print(\"DataFrame-1\")\n",
    "print(df1.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|Purchase|Purchase_new|\n",
      "+--------+------------+\n",
      "|    8370|      4185.0|\n",
      "|   15200|      7600.0|\n",
      "|    1422|       711.0|\n",
      "|    1057|       528.5|\n",
      "|    7969|      3984.5|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To add the new column in DataFrame\n",
    "## Use withColumn operation to add new column (we can also replace) in base DataFrame and return a new DataFrame. \n",
    "## The withColumn operation will take 2 parameters.\n",
    "## Column name to be added /replaced.\n",
    "## Expression on column.\n",
    "\n",
    "## Derive new column, ‘Purchase_new’ in train which is calculated by dviding Purchase column by 2.\n",
    "\n",
    "trainDF.withColumn('Purchase_new', trainDF.Purchase /2.0).select('Purchase','Purchase_new').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To drop a column in DataFrame\n",
    "## To drop a column from the DataFrame use drop operation. \n",
    "## Drop the column called ‘Comb’ from the test and get the remaining columns in test dataframe\n",
    "testDF.drop('Comb').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To remove some categories of Product_ID column in test that are not present in Product_ID column in train\n",
    "## Use an user defined function ( udf ) to remove the categories of a column which are in test but not in train.\n",
    "## Calculate the categories in Product_ID column which are in test but not in train.\n",
    "diff_cat_in_train_test=testDF.select('Product_ID').subtract(trainDF.select('Product_ID'))\n",
    "diff_cat_in_train_test.count() # For distict count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Product_ID|\n",
      "+----------+\n",
      "| P00322642|\n",
      "| P00300142|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_cat_in_train_test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "<type 'list'>\n",
      "[u'P00322642', u'P00300142', u'P00077642', u'P00249942', u'P00294942', u'P00106242', u'P00239542', u'P00074942', u'P00092742', u'P00082142', u'P00030342', u'P00062542', u'P00063942', u'P00013042', u'P00279042', u'P00227242', u'P00359842', u'P00061642', u'P00042642', u'P0099542', u'P00306842', u'P00140842', u'P00165542', u'P00322842', u'P00268942', u'P00236842', u'P00038942', u'P00172942', u'P00012642', u'P00270342', u'P00312642', u'P00336842', u'P00105742', u'P00309842', u'P00166542', u'P00082642', u'P00253842', u'P00062242', u'P00100242', u'P00315342', u'P00058842', u'P00168242', u'P00156942', u'P00039042', u'P00056942', u'P00204642']\n"
     ]
    }
   ],
   "source": [
    "## There are 46 different categories in test. \n",
    "## To remove these categories from the test ‘Product_ID’ column.\n",
    "\n",
    "## Create the distinct list of categories called ‘not_found_cat’ from the diff_cat_in_train_test using map operation.\n",
    "## Register a udf(user define function).\n",
    "## User defined function will take each element of test column and search this in not_found_cat list and \n",
    "## it will put -1 ifit finds in this list otherwise it will do nothing.\n",
    "not_found_cat = diff_cat_in_train_test.rdd.map(lambda x: x[0]).collect()\n",
    "print(len(not_found_cat))\n",
    "print(type(not_found_cat))\n",
    "print(not_found_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Defined Functions - UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Resister the udf, we need to import StringType from the pyspark.sql and udf from the pyspark.sql.functions. \n",
    "## The udf function takes 2 parameters as arguments:\n",
    "## Return type (in my case StringType())\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "Function1 = udf(lambda x: '-1' if x in not_found_cat else x, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|NEW_Product_ID|\n",
      "+--------------+\n",
      "|            -1|\n",
      "|            -1|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## In the above code function name is ‘Function1’ and we are putting ‘-1’  for not found catagories in test ‘Product_ID’. \n",
    "## Finally apply above ‘Function1’ function on test ‘Product_ID’ and take result in k for new column calles “NEW_Product_ID”.\n",
    "\n",
    "k = testDF.withColumn(\"NEW_Product_ID\",Function1(testDF[\"Product_ID\"])).select('NEW_Product_ID')\n",
    "k.where(k['NEW_Product_ID'] == -1).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## See the results by again calculating the different categories in k and train subtract operation.\n",
    "diff_cat_in_train_test=k.select('NEW_Product_ID').subtract(trainDF.select('Product_ID'))\n",
    "print(diff_cat_in_train_test.count())# For distinct count\n",
    "print(diff_cat_in_train_test.distinct().count())# For distinct count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(NEW_Product_ID=u'-1')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The output 1 means we have now only 1 different category k and train.\n",
    "diff_cat_in_train_test.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "trainDF.select(round(lit(\"2.5\")), bround(lit(2.5))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|            3|             2|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT round(2.5), bround(2.5)\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.34370334592\n",
      "+----------------------------------+\n",
      "|corr(Purchase, Product_Category_1)|\n",
      "+----------------------------------+\n",
      "|              -0.34370334591990875|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "print(trainDF.stat.corr(\"Purchase\", \"Product_Category_1\"))\n",
    "trainDF.select(corr(\"Purchase\", \"Product_Category_1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|corr(CAST(Purchase AS DOUBLE), CAST(Product_Category_1 AS DOUBLE))|\n",
      "+------------------------------------------------------------------+\n",
      "|                                              -0.34370334591990875|\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT corr(Purchase, Product_Category_1) FROM trainDFTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|Age_freqItems                                 |\n",
      "+----------------------------------------------+\n",
      "|[26-35, 18-25, 55+, 46-50, 51-55, 36-45, 0-17]|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.stat.freqItems([\"Age\"]).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatalystOptimizer.ipynb  Spark_DataFrames_SQL-Copy1.ipynb\r\n",
      "SalesData\t\t Spark_DataFrames_SQL.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfsroot/data/home/mahidharv/academics/Batch44/CSE7322c/20181007_Batch 44_CSE 7322c_SparkSQL and DataFrames_Lab\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat.jpg  test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls SalesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/04/13 21:57:18 INFO fs.TrashPolicyDefault: Moved: 'hdfs://bigdata/user/mahidharv/SalesData' to trash at: hdfs://bigdata/user/mahidharv/.Trash/Current/user/mahidharv/SalesData\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/mahidharv/SalesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/mahidharv/SalesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -chmod 777 /user/mahidharv/SalesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/home/mahidharv/academics/Batch44/CSE7322c/20181007_Batch': No such file or directory\r\n",
      "copyFromLocal: `44_CSE': No such file or directory\r\n",
      "copyFromLocal: `7322c_SparkSQL': No such file or directory\r\n",
      "copyFromLocal: `and': No such file or directory\r\n",
      "copyFromLocal: `DataFrames_Lab/SalesData/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyFromLocal /home/mahidharv/academics/Batch44/CSE7322c/20181007_Batch 44_CSE 7322c_SparkSQL and DataFrames_Lab/SalesData/* /user/mahidharv/SalesData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---+----------+\n",
      "| ltrim| rtrim| trim| lp|        rp|\n",
      "+------+------+-----+---+----------+\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "+------+------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "\n",
    "trainDF.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\"))\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------+---------------------+----------------------+\n",
      "|ltrim( HELLLOOOO )|rtrim( HELLLOOOO )|trim( HELLLOOOO )|lpad(HELLOOOO , 3,  )|rpad(HELLOOOO , 10,  )|\n",
      "+------------------+------------------+-----------------+---------------------+----------------------+\n",
      "|        HELLLOOOO |         HELLLOOOO|        HELLLOOOO|                  HEL|            HELLOOOO  |\n",
      "|        HELLLOOOO |         HELLLOOOO|        HELLLOOOO|                  HEL|            HELLOOOO  |\n",
      "+------------------+------------------+-----------------+---------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT\n",
    "ltrim(' HELLLOOOO '),\n",
    "rtrim(' HELLLOOOO '),\n",
    "trim(' HELLLOOOO '),\n",
    "lpad('HELLOOOO ', 3, ' '),\n",
    "rpad('HELLOOOO ', 10, ' ')\n",
    "FROM\n",
    "trainDFTable\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "| Gender_DECODE|Gender|\n",
      "+--------------+------+\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "+--------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"F|M\"\n",
    "\n",
    "trainDF.select(\n",
    "regexp_replace(col(\"Gender\"), regex_string, \"MALE_OR_FEMALE\")\n",
    ".alias(\"Gender_DECODE\"),\n",
    "col(\"Gender\"))\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "| Gender_DECODE|Gender|\n",
      "+--------------+------+\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "+--------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "regexp_replace(Gender, 'F|M', 'MALE_OR_FEMALE') as\n",
    "Gender_DECODE,\n",
    "Gender\n",
    "FROM\n",
    "trainDFTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|translate(Gender, FM, 01)|Gender|\n",
      "+-------------------------+------+\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "+-------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "trainDF.select(\n",
    "translate(col(\"Gender\"), \"FM\", \"01\"),\n",
    "col(\"Gender\"))\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|translate(Gender, FM, 01)|Gender|\n",
      "+-------------------------+------+\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        0|     F|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "|                        1|     M|\n",
      "+-------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "translate(Gender, 'FM', '01'),\n",
    "Gender\n",
    "FROM\n",
    "trainDFTable\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2019-04-13|2019-04-13 21:57:...|\n",
      "|  1|2019-04-13|2019-04-13 21:57:...|\n",
      "|  2|2019-04-13|2019-04-13 21:57:...|\n",
      "|  3|2019-04-13|2019-04-13 21:57:...|\n",
      "|  4|2019-04-13|2019-04-13 21:57:...|\n",
      "|  5|2019-04-13|2019-04-13 21:57:...|\n",
      "|  6|2019-04-13|2019-04-13 21:57:...|\n",
      "|  7|2019-04-13|2019-04-13 21:57:...|\n",
      "|  8|2019-04-13|2019-04-13 21:57:...|\n",
      "|  9|2019-04-13|2019-04-13 21:57:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.createOrReplaceTempView(\"dateDFTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2019-04-08|        2019-04-18|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col(\"today\"), 5),date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2019-04-08|        2019-04-18|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "date_sub(today, 5),\n",
    "date_add(today, 5)\n",
    "FROM\n",
    "dateDFTable\n",
    "\"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "dateDF\\\n",
    ".withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\")))\\\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|months_between(start, end)|\n",
      "+--------------------------+\n",
      "|               -13.5483871|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF\\\n",
    ".select(\n",
    "to_date(lit(\"2017-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2018-02-18\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\")))\\\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------------------------------------------------------------+------------------------------------------------------------+\n",
      "|to_date('2016-01-01')|months_between(CAST(2016-01-01 AS TIMESTAMP), CAST(2017-01-01 AS TIMESTAMP))|datediff(CAST(2016-01-01 AS DATE), CAST(2017-01-01 AS DATE))|\n",
      "+---------------------+----------------------------------------------------------------------------+------------------------------------------------------------+\n",
      "|           2016-01-01|                                                                       -12.0|                                                        -366|\n",
      "|           2016-01-01|                                                                       -12.0|                                                        -366|\n",
      "+---------------------+----------------------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "to_date('2016-01-01'),\n",
    "months_between('2016-01-01', '2017-01-01'),\n",
    "datediff('2016-01-01', '2017-01-01')\n",
    "FROM\n",
    "dateDFTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
    ".select(to_date(col(\"date\")))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WARNING__\n",
    "<br>Spark will not throw an error if it cannot parse the date, it’ll just return null. This can be a bit tricky in larger pipelines because you may be expecting your data in one format and getting it in another. To illustrate, let’s take a look at the date format that has switched from year-month-day to year-day-month. Spark will fail to parse this date and silently return null instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 2016-20-12 - year-day-month\n",
    "### 2017-12-11 - year-month-day\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "\n",
    "cleanDateDF = spark.range(1)\\\n",
    ".select(to_date(unix_timestamp(lit(\"2017-12-11\"), dateFormat)\n",
    ".cast(\"timestamp\"))\\\n",
    ".alias(\"date\"),\n",
    "to_date(unix_timestamp(lit(\"2017-20-12\"), dateFormat)\n",
    ".cast(\"timestamp\"))\\\n",
    ".alias(\"date2\"))\n",
    "\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------+\n",
      "|to_date(CAST(unix_timestamp(datetable2.`date`, 'yyyy-dd-MM') AS TIMESTAMP))|to_date(CAST(unix_timestamp(datetable2.`date2`, 'yyyy-dd-MM') AS TIMESTAMP))|to_date(datetable2.`date`)|\n",
      "+---------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------+\n",
      "|                                                                 2017-11-12|                                                                  2017-12-20|                2017-11-12|\n",
      "+---------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "to_date(cast(unix_timestamp(date, 'yyyy-dd-MM') as timestamp)),\n",
    "to_date(cast(unix_timestamp(date2, 'yyyy-dd-MM') as timestamp)),\n",
    "to_date(date)\n",
    "FROM\n",
    "dateTable2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|CAST(unix_timestamp(date, yyyy-dd-MM) AS TIMESTAMP)|\n",
      "+---------------------------------------------------+\n",
      "|                                2017-11-12 00:00:00|\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF\\\n",
    ".select(unix_timestamp(col(\"date\"), dateFormat).cast(\"timestamp\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|        Description|\n",
      "+---+-------------------+\n",
      "|  0|This is long string|\n",
      "|  1|This is long string|\n",
      "|  2|This is long string|\n",
      "|  3|This is long string|\n",
      "|  4|This is long string|\n",
      "|  5|This is long string|\n",
      "|  6|This is long string|\n",
      "|  7|This is long string|\n",
      "|  8|This is long string|\n",
      "|  9|This is long string|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textDF = spark.range(10).withColumn(\"Description\", lit(\"This is long string\"))\n",
    "textDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|split(Description,  )|\n",
      "+---------------------+\n",
      "| [This, is, long, ...|\n",
      "| [This, is, long, ...|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "textDF.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|split(Description,  )|\n",
      "+---------------------+\n",
      "| [This, is, long, ...|\n",
      "| [This, is, long, ...|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textDF.createOrReplaceTempView('textDFTable')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "split(Description, ' ')\n",
    "FROM\n",
    "textDFTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the function is created, we need to register them with Spark so that we can used\n",
    "them on all of our worker machines. Spark will serialize the function on the driver, and transfer it over the network to all executor processes. This happens regardless of language.\n",
    "\n",
    "<br>Once we go to use the function, there are essentially two different things that occur. If the function is written in Scala or Java then we can use that function within the JVM. This means there will be little performance penalty aside from the fact that we can’t take advantage of code generation capabilities that Spark has for built-in functions.\n",
    "\n",
    "<br>If the function is written in Python, something quite different happens. \n",
    "Spark will start up a python process on the worker, serialize all of the data to a format that python can understand (remember it was in the JVM before), execute the function row by row on that data in the python process, before finally returning the results of the row operations to the JVM and Spark.\n",
    "\n",
    "![UDF_Spark_Python](./Images/UDF_Spark_Python.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF Written in Scala\n",
    "#### Try in spark-shell\n",
    "val udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "\n",
    "def power3(number:Double):Double = {\n",
    "<br>          number X number X number\n",
    "<br>}\n",
    "\n",
    "power3(2.0)\n",
    "\n",
    "![UDF_Scala_PySpark](./Images/Scala_UDF.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myList = [('Alpha',25),('Beta',22),('Charlie',20),('Delta',22), ('Echo',21),('France',22),('Gamma',23)]\n",
    "rdd = spark.sparkContext.parallelize(myList)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=25, name='Alpha'),\n",
       " Row(age=22, name='Beta'),\n",
       " Row(age=20, name='Charlie'),\n",
       " Row(age=22, name='Delta'),\n",
       " Row(age=21, name='Echo'),\n",
       " Row(age=22, name='France'),\n",
       " Row(age=23, name='Gamma')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaPeople = spark.createDataFrame(people)\n",
    "\n",
    "# check the type of schemaPeople.\n",
    "type(schemaPeople)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 25|  Alpha|\n",
      "| 22|   Beta|\n",
      "| 20|Charlie|\n",
      "| 22|  Delta|\n",
      "| 21|   Echo|\n",
      "| 22| France|\n",
      "| 23|  Gamma|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Shared Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_collection = \"Postgraduate Program in Big Data Analytics and Optimization\"\\\n",
    "  .split(\" \")\n",
    "    \n",
    "words = spark.sparkContext.parallelize(my_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Postgraduate', 'Program', 'in']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "supplementalData = {\"Postgraduate\":1000, \"Analytics\":200, \"Optimization\": 400,\n",
    "                    \"Big\":-300, \"Data\": 100, \"Program\":100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "suppBroadcast = spark.sparkContext.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Analytics': 200,\n",
       " 'Big': -300,\n",
       " 'Data': 100,\n",
       " 'Optimization': 400,\n",
       " 'Postgraduate': 1000,\n",
       " 'Program': 100}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Big', -300),\n",
       " ('in', 0),\n",
       " ('and', 0),\n",
       " ('Program', 100),\n",
       " ('Data', 100),\n",
       " ('Analytics', 200),\n",
       " ('Optimization', 400),\n",
       " ('Postgraduate', 1000)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: (word, suppBroadcast.value.get(word, 0)))\\\n",
    "  .sortBy(lambda wordPair: wordPair[1])\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cwgDF = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(\"file:///home/mahidharv/SparkSql_Activity/XXI_Commonwealth_Games.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+----+------+------+-----+\n",
      "|Seq|NationCode|  NationName|Gold|Silver|Bronze|Total|\n",
      "+---+----------+------------+----+------+------+-----+\n",
      "|  1|       AUS|   Australia|  60|    45|    46|  151|\n",
      "|  2|       ENG|     England|  28|    31|    24|   83|\n",
      "|  3|       IND|       India|  14|     6|     9|   29|\n",
      "|  4|       CAN|      Canada|  11|    26|    19|   56|\n",
      "|  5|       RSA|South Africa|  11|     9|    12|   32|\n",
      "+---+----------+------------+----+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cwgDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Seq,IntegerType,true),StructField(NationCode,StringType,true),StructField(NationName,StringType,true),StructField(Gold,IntegerType,true),StructField(Silver,IntegerType,true),StructField(Bronze,IntegerType,true),StructField(Total,IntegerType,true)))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwgDF.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accIND = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accINDFunc(each_row):\n",
    "    countryCD = each_row[\"NationCode\"]\n",
    "    list_ctrys = [\"IND\", \"SRI\", \"PAK\", \"BAN\"]\n",
    "    if countryCD in list_ctrys:\n",
    "        accIND.add(each_row[\"Total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwgDF.foreach(lambda each_row: accINDFunc(each_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=38>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accIND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1309px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
