---
title: "Multiple Linear Regression - Predicting the Prices of house in Boston"
author: "INSOFE Lab"
date: "14th July 2018"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
---

* Objective - Build a multiple linear regression model using lm()

**NOTE** Before starting this assignment please remember to clear your environment, you can do that by running the following code chunk. 

```{r}

rm(list = ls(all=TRUE))

```

# Agenda 

* Get the data

* Explore the data

* Data Pre-processing

* Model the data

* Evaluation


# Reading & Understanding the Data

* Make sure the dataset is located in your current working directory

```{r}

housing_data <- read.csv(file = "HousingData.csv",header = T,sep = ',')

```

* Use the str() function to get a feel for the dataset.

```{r}

str(housing_data)

```

* The dataset has 506 rows and 14 columns.

* The column/variable names' explanation is given below:

1) __CRIM :__ Per capita Crime rate by town

2) __ZN :__ Proportion of residential land zoned for lots over 25,000 sq.ft.

3) __INDUS :__ Proportion of non-retail business acres per town

4) __CHAS :___ Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)

5) __NOX :__ nitric oxides concentration (parts per 10 million)

6) __RM :__ average number of rooms per dwelling

7) __AGE :__ proportion of owner-occupied units built prior to 1940

8) __DIS :__ weighted distances to five Boston employment centres

9) __RAD :__ index of accessibility to radial highways

10) __TAX :__ full-value property-tax rate per $10,000

11) __PTRATIO :__ pupil-teacher ratio by town

12) __B :__ 1000(Bk - 0.63)^2 where Bk is the proportion of African-Americans by town

13) __LSTAT :__ Percentage of the population in the lower economic status 

14) __MEDV  :__ Median value of owner-occupied homes in multiples of $1000


* Take a look at the data using the "head()" and "tail()" functions

```{r}

head(housing_data)
```

```{r}

tail(housing_data)

```
* So, let's get ready to predict the prices of houses!!

# Exploratory Analysis

## Summary Statistics

* Understand the distribution of various variables in the datset using the "summary()" function

```{r}

summary(housing_data)
```
## Study the NA values per column
```{r}
data.frame(colSums(is.na(housing_data)))
```


## Scatter Plots

* A few bi-variate relationships are plotted below, but you are encouraged to explore the dataset in more detail

```{r fig.height= 8, fig.width = 9}

par(mfrow = c(2,3))

plot(housing_data$LSTAT, housing_data$MV, ylab = "Median House Price", xlab = "Percentage of people in the lower economic st", main = "Housing Price vs Status")

plot(housing_data$CRIM, housing_data$MV, ylab = "Median House Price", xlab = "Per capita crime by town", main = "Housing Price vs Per Capita Crime")

plot(housing_data$NOX, housing_data$MV, ylab = "Median House Price", xlab = "Nitric Oxide Concentration in ppm", main = "Housing Price vs NOX concentration in ppm")

plot(housing_data$INDUS, housing_data$MV, ylab = "Median House Price", xlab = "Proportion of non-retail business acres per town", main = "Housing Price vs Non-retail business area")

boxplot(as.factor(housing_data$CHAS),housing_data$MV,names =levels(housing_data$CHAS))
```

* Report your observation looking at above plots

## Correlation Plot

* Let's have a look at the various correlations between the variables in the dataset

```{r fig.height= 8, fig.width = 9}
#install.packages("corrplot")
library(corrplot)

correlation_XPairwise = cor(housing_data)
corrplot(cor(housing_data, use = "complete.obs"), method = "number") # If use is "complete.obs" then missing values are handled by casewise deletion

# Write the pairwise correlation matrix to a csv file for later analysis
write.table(correlation_XPairwise,file="pairwiseCorrelations1.csv",row.names=FALSE,col.names=FALSE,sep=",")
## Observation find a  mistake in the plot below
```
* Observe the variables which are highly correlated (RAD and TAX)

# Data Pre-processing

* Today we will impute missing values and standardize the data __after__ splitting the data into train and test sets

## Convert the data type into require format
```{r}
housing_data$CHAS = as.factor(housing_data$CHAS)
```

## Train/Test Split

* 70/30 - Train/Test split

```{r}

set.seed(123)

# the "sample()" function helps us to randomly sample 70% of the row indices of the dataset

train_rows <- sample(x = 1:nrow(housing_data), size = 0.7*nrow(housing_data))

# We use the above indices to subset the train and test sets from the data

train_data <- housing_data[train_rows, ]

test_data <- housing_data[-train_rows, ]

dim(train_data)
dim(test_data)
```

## Columnwise Missing values in Housing Data
```{r}
cat("Missing Data Per Columns in Train Data \n")
colSums(is.na(train_data))
cat("Missing Data Per Columns in Test Data \n")
colSums(is.na(test_data))

```

## Missing Values Imputation

* Find out the number of missing values in the dataset

* Impute the missing values using the "preProcess()" function in conjunction with the "medianImpute" method

```{r}
#install.packages("caret")
library(caret)

sum(is.na(train_data))
sum(is.na(test_data))

imputer_values <- preProcess(x = train_data[, !names(train_data) %in% c("CHAS")], method = "medianImpute")

train_data[, !names(train_data) %in% c("CHAS")] <- predict(object = imputer_values, newdata = train_data[, !names(train_data) %in% c("CHAS")])
sum(is.na(train_data[, !names(train_data) %in% c("CHAS")]))

test_data[, !names(train_data) %in% c("CHAS")] <- predict(object = imputer_values, newdata = test_data[, !names(train_data) %in% c("CHAS")])

sum(is.na(test_data[, !names(train_data) %in% c("CHAS")]))

sum(is.na(train_data))
sum(is.na(test_data))
```
## Imputing the Categorical attributes with the Mode
```{r}
library(DMwR)
centralValue(train_data$CHAS)
train_data = centralImputation(train_data)

```

## Sanity Checking for the missin gvalues in both train and test
```{r}
cat("Missing Data Per Columns in Train Data after imputation \n")
colSums(is.na(train_data))
cat("Missing Data Per Columns in Test Data after imputation \n")
colSums(is.na(test_data))
```

## Standardizing the Data

* We will use the Caret pakcage to standardize the data after the split using the __"preProcess()"__ function

* It saves the metrics such as mean and standard deviation used for calculating the standardized value by creating a model object

* We can then use the model object in the __"predict()"__ function to standardize any other unseen dataset with the same distribuiton and variables

```{r}
# The "preProcess()" function creates a model object required for standardizing unseen data

# Do not standardize the target variable

std_model <- preProcess(train_data[, !names(train_data) %in% c("MV","CHAS")], method = c("center", "scale"))
std_model
# The predict() function is used to standardize any other unseen data

train_data[, !names(train_data) %in% c("MV","CHAS")] <- predict(object = std_model, newdata = train_data[, !names(train_data) %in% c("MV","CHAS")])

test_data[, !names(train_data) %in% c("MV","CHAS")] <- predict(object = std_model, newdata = test_data[, !names(train_data) %in% c("MV","CHAS")])

summary(train_data)


```



# Modelling the Data

## Basic Model

* The "." adds all the variables other than the response variable while building the model.

```{r}
# Capturing the Start and end time taken for model building
start_time = Sys.time()
model_basic <- lm(formula = MV~. , data =(train_data))
end_time = Sys.time()
time_taken = end_time - start_time
time_taken
cat(end_time - start_time)

```
## Model Summary 
```{r}
summary(model_basic)
```

```{r}
df_residuals =model_basic$residuals
df_residuals
```

```{r}
Residuals = function(model,data,target){
    predicted = predict(model,data)
    residuals = target - predicted
    return (residuals)
}
dfResiduals = Residuals(model_basic,train_data,train_data$MV)
```

```{r}
summary(dfResiduals)
```
## Standard Error of the Residuals
```{r}
SE_residuals =function(residuals,numberofparameters){
  squareOfResiduals = residuals^2
  sumOfResiduals = sum(squareOfResiduals)
  deg_of_freedom = length(residuals)-(numberofparameters)-1  
  standardErrorResiduals = sqrt(sumOfResiduals/deg_of_freedom)
  cat("Residual standard error:",standardErrorResiduals," on",deg_of_freedom," degrees of freedom")
}
```

```{r}
standardErrorResiduals = SE_residuals(dfResiduals,13)
```

## Adjusted R squared
```{r}
Multiple_Adjusted_Rsquared = function(data,model,numberofparameters,target){
  yhat = predict(model,data)
  y = target
 
  multiple_Rsquared = round(1 - sum((y-yhat)^2)/sum((y-mean(y))^2),digits=4)
  
  cat("\n Mutliple R Squared :", multiple_Rsquared)
  adjustedRsquared = 1-(1-multiple_Rsquared)*((nrow(data)-1)/(nrow(data)-(numberofparameters)-1))
  cat("\n Adjusted R Square: ",round(adjustedRsquared,digits = 4))
}
Multiple_Adjusted_Rsquared(train_data,model_basic,13,train_data$MV)
```


* Question: How good is the model?
*	A basic assessment of the model can be obtained by reading off the R^2 (and adjusted R^2) values.
*	Caution : A good R^2 value alone can be misleading.
	
* Question: Is the model significant?
*	Important : Both model significance as well as significance of the individual coefficients need to be considered.

## Residual Plots
* Interpreting the diagnostic plots to check for the linear regression assumptions
```{r}
par(mfrow = c(2,2))

plot(model_basic)

```

# Influential Observations 
## Leverage


```{r}
#install.packages("car")
library(car)
lev= hat(model.matrix(model_basic))
plot(lev)
hlev = which(lev>0.3)
max=as.numeric(which.max(lev))
points(max,lev[max],col='red', pch=6)
text(lev,labels = rownames(train_data))



```

* Suppose we would like to remove records with leverage values greater than 0.3

```{r}

train_data[lev>0.3,]
train_data_lev<-train_data[-(lev>0.3),]
dim(train_data_lev)

## Method: Convention - If there are n data points and p parameters, then threshold can be taken a 3*p/n
 # lev_threshold <- 3*length(model_basic$coefficients)/length(lev)
 # train_data[which(lev>lev_threshold)]
 # length(train[lev>lev_threshold,])

```
# Cook's distance 
* Identifying influential observations and handling them
```{r}

# Identify records with high Cook's distance

cook = cooks.distance(model_basic)
plot(cook,ylab="Cook's distances")
max=as.numeric(which.max(cook))
max
points(max,cook[max],col='red', pch=19)
text(seq(1:nrow(train_data)),cook,labels = rownames(train_data))

train_cook<-train_data_lev[-max,]
dim(train_cook)

```
* Model building after removing influential observations

```{r}

model_basic2 <- lm(formula = MV~. , data = train_cook)
summary(model_basic2)

```
* Observation: Adjusted R^2 did not change much

# stepAIC model

* "stepAIC()" is a function in the MASS package

* stepAIC uses AIC (Akaike information criterion) to either drop variables ("backward" direction) or add variables ("forward" direction) from the model

```{r}

library(MASS)

model_aic <- stepAIC(model_basic2, direction = "both",trace=TRUE)

summary(model_aic)

par(mfrow = c(2,2))

plot(model_aic)

```

## Modifying the Model with the VIF

**Variance Inflation Factor :**
** Checking for multicollinearity through VIF.

$$VIF_{k} = \dfrac{1}{1 - R_{k}^2}$$

$R_{k}^2$ is the R^2-value obtained by regressing the kth predictor on the remaining predictors. VIF gives us an idea of multi-collinearity

* Every explanatory variable would have a VIF score

* A VIF > 4 means that there are signs of multi-collinearity and anything greater than 10 means that an explanatory variable should be dropped

* We use the "vif()" function from the car package. 

```{r}

library(car)

vif(model_basic)

vif(model_aic)

```

* After applying the stepAIC, the VIF values have slightly reduced, but the variables "RAD" and "TAX" have VIF values higher than 4

* Let's take a look at the correlation between the "RAD" and "TAX" variables

```{r}

cor(housing_data$RAD, housing_data$TAX,use='complete.obs')

```

* The correlation coefficient is extremely high between the "RAD" and "TAX" variables

* let's now remove the "TAX" variable, as it is the lesser significant of the two

* Build another model without the "TAX" variable, and take a look at the VIF


```{r}

model3 <- lm(formula = MV ~ CRIM + ZN + CHAS + NOX + RM + DIS + RAD + PT + B + LSTAT, data = train_data)
summary(model3)

par(mfrow = c(2,2))

plot(model3)

vif(model3)

```

# Evaluation and Selection of Model

## Picking the Right Model

* The third model built after verifying the vif scores has a similar adjusted R^2 score compared to the previous models with significantly lower no. of explanatory variables and inter-variable interactions.

* The VIF values of the predictors in the third model are lower when compared to the the other two models

* Due to the above two reasons we pick the third model

# Communication

## Prediction

Predict the Housing prices of the unseen boston housing data, using the chosen model.

```{r}

preds_model <- predict(model3, test_data[, !(names(test_data) %in% c("MV"))])
preds_model
```

## Performance Metrics

Once we choose the model we have to report performance metrics on the test data. We are going to report four error metrics for regression.

### Error Metrics for Regression

* Mean Absolute Error (MAE):

$$MAE = \dfrac{1}{n}\times\sum_{i = 1}^{n}|y_{i} - \hat{y_{i}}|$$


* Mean Squared Error (MSE):

$$MSE = \dfrac{1}{n}\times\sum_{i = 1}^{n}(y_{i} - \hat{y_{i}})^2$$


* Root Mean Squared Error (RMSE):

$$RMSE = \sqrt{\dfrac{1}{n}\times\sum_{i = 1}^{n}(y_{i} - \hat{y_{i}})^2}$$


* Mean Absolute Percentage Error (MAPE):

$$MAPE = \dfrac{100}{n}\times\dfrac{\sum_{i = 1}^{n}\mid{y_{i} - \hat{y_{i}}}\mid}{y_{i}}$$


### Report Performance Metrics

* Report performance metrics obtained by using the chosen model on the test data

## Test Data

### Evaluating the Model

```{r}
library(DMwR)

# Error verification on train data

regr.eval(train_data$MV, model3$fitted.values)

```

```{r}

# Error verification on test data

regr.eval(test_data$MV, preds_model)

```

* Observation: Error Metrics is not far apart for train and test data after its evaluated on the third model

* Future Scope: 
  * i) More models can be tried to improve Adjusted R^2
  * ii) Data Transformation can be applied 

















































